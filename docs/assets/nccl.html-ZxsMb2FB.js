import{_ as s,c as i,o as n,a}from"./app-DpNcotM2.js";const e={},l=a(`<div class="hint-container tip"><p class="hint-container-title">nccl</p></div><h2 id="nccl" tabindex="-1"><a class="header-anchor" href="#nccl"><span>nccl</span></a></h2><p>NCCL是一款独立的库，提供标准的 GPU 通信例程，支持全规约（all-reduce）、全收集（all-gather）、规约（reduce）、广播（broadcast）、规约并散播（reduce-scatter）以及任意基于发送/接收的通信模式。该库经过优化，能够在使用 PCIe、NVLink、NVSwitch 以及基于 InfiniBand Verbs 或 TCP/IP 套接字的网络平台上实现高带宽。NCCL 支持任意数量的 GPU，无论是安装在单节点还是跨多个节点的系统中，并且可以在单进程或多进程（如 MPI）应用程序中使用。</p><p>NCCL 概述<br> NVIDIA 集体通信库（NCCL，发音为“Nickel”）是一个库，提供拓扑感知的 GPU 间通信原语，能够方便地集成到应用程序中。</p><p>NCCL 实现了集体通信和点对点发送/接收原语。它不是一个完整的并行编程框架，而是一个专注于加速 GPU 间通信的库。</p><p>NCCL 提供以下集体通信原语：</p><ul><li>AllReduce（全规约）</li><li>Broadcast（广播）</li><li>Reduce（规约）</li><li>AllGather（全收集）</li><li>ReduceScatter（规约并散播）</li></ul><p>此外，它还支持点对点发送/接收通信，允许实现 scatter、gather 或 all-to-all 操作。</p><p>在集体通信中，通信处理器之间的紧密同步是关键。传统上，基于 CUDA 的集体操作通常通过 CUDA 内存拷贝操作和 CUDA 核函数结合本地规约来实现。而 NCCL 则将每个集体操作实现为一个处理通信和计算操作的单一核函数。这样可以实现快速同步，并最大限度地减少达到峰值带宽所需的资源。</p><p>NCCL 方便地免除了开发人员为特定机器优化应用程序的需求。NCCL 在多个 GPU 之间提供快速的集体通信，无论是在单个节点内还是跨节点。它支持多种互连技术，包括 PCIe、NVLINK、InfiniBand Verbs 和 IP 套接字。</p><p>除了性能，简便的编程体验也是 NCCL 设计的主要考虑因素之一。NCCL 使用简单的 C API，可以轻松通过多种编程语言访问。NCCL 紧密遵循 MPI（消息传递接口）定义的流行集体操作 API，因此熟悉 MPI 的用户会发现 NCCL 的 API 非常容易使用。与 MPI 略有不同的是，NCCL 的集体操作带有一个“流”参数，使其能够直接与 CUDA 编程模型集成。最后，NCCL 几乎兼容任何多 GPU 并行化模型，例如：</p><ul><li>单线程控制所有 GPU</li><li>多线程（例如，每个 GPU 一个线程）</li><li>多进程（例如，MPI）</li></ul><p>NCCL 在深度学习框架中有着广泛的应用，AllReduce 集体操作在神经网络训练中被广泛使用。通过 NCCL 提供的多 GPU 和多节点通信，可以实现神经网络训练的高效扩展。</p><p>NCCL 是一个通信库，提供用于高性能应用的 GPU 间优化通信。与 MPI 不同，NCCL 不提供并行环境，也不包含进程启动器和管理器。因此，NCCL 依赖于应用程序的进程管理系统和 CPU 端的通信系统来进行自启动。</p><p>与 MPI 和其他为性能优化的库类似，NCCL 不提供 GPU 之间的安全网络通信。因此，用户有责任确保 NCCL 在安全的网络上运行，无论是在自启动阶段（由 NCCL_SOCKET_IFNAME 控制）还是在高速通信过程中。</p><h3 id="下载源码" tabindex="-1"><a class="header-anchor" href="#下载源码"><span>下载源码</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>git clone https://github.com/NVIDIA/nccl-tests.git</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="构建编译" tabindex="-1"><a class="header-anchor" href="#构建编译"><span>构建编译</span></a></h3><ul><li><p>修改换行格式 LF</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>src\\device\\generate.py</span></span>
<span class="line"><span></span></span>
<span class="line"><span>src\\device\\Makefile</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>编译并安装</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span> make -j src.build</span></span>
<span class="line"><span> make install</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>构建产物如下</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>├── include</span></span>
<span class="line"><span>│   ├── nccl.h</span></span>
<span class="line"><span>│   └── nccl_net.h</span></span>
<span class="line"><span>├── lib</span></span>
<span class="line"><span>    ├── libnccl.so -&gt; libnccl.so.2</span></span>
<span class="line"><span>    ├── libnccl.so.2 -&gt; libnccl.so.2.23.4</span></span>
<span class="line"><span>    ├── libnccl.so.2.23.4</span></span>
<span class="line"><span>    ├── libnccl_static.a</span></span>
<span class="line"><span>    └── pkgconfig</span></span>
<span class="line"><span>        └── nccl.pc</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><h2 id="nccl-test" tabindex="-1"><a class="header-anchor" href="#nccl-test"><span>nccl-test</span></a></h2><h3 id="下载" tabindex="-1"><a class="header-anchor" href="#下载"><span>下载</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>git clone https://github.com/NVIDIA/nccl-tests.git</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="编译" tabindex="-1"><a class="header-anchor" href="#编译"><span>编译</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>cd nccl-tests </span></span>
<span class="line"><span>make</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="运行" tabindex="-1"><a class="header-anchor" href="#运行"><span>运行</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>export NCCL_DEBUG=TRACE</span></span>
<span class="line"><span>./build/all_reduce_perf -b 8 -e 8M -f 2 -g 1</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>(base) root@DESKTOP-P54EAF3:/mnt/d/Code/nccl-tests# ./build/all_reduce_perf -b 8 -e 8M -f 2 -g 1</span></span>
<span class="line"><span># nThread 1 nGpus 1 minBytes 8 maxBytes 8388608 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0</span></span>
<span class="line"><span>#</span></span>
<span class="line"><span># Using devices</span></span>
<span class="line"><span>#  Rank  0 Group  0 Pid  58239 on DESKTOP-P54EAF3 device  0 [0x01] NVIDIA GeForce GTX 1660 SUPER</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58239 [0] NCCL INFO Bootstrap : Using eth0:172.26.190.235&lt;0&gt;</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58239 [0] NCCL INFO cudaDriverVersion 12050</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58239 [0] NCCL INFO NCCL version 2.23.4+cuda12.1</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal network plugin.</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Failed to open libibverbs.so[.1]</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO NET/Socket : Using [0]eth0:172.26.190.235&lt;0&gt; [1]br-eaee979aadb0:172.19.0.1&lt;0&gt; [2]vethd14a450:fe80::c821:6bff:fea5:dcf9%vethd14a450&lt;0&gt; [3]vethc5ec3b7:fe80::74f5:23ff:fe0a:3d6d%vethc5ec3b7&lt;0&gt;</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Using network Socket</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO ncclCommInitAll comm 0x559205409610 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1000 commId 0x89b784d33d72ebbb - Init START</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Bootstrap timings total 0.000344 (create 0.000019, send 0.000105, recv 0.000096, ring 0.000000, delay 0.000000)</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO comm 0x559205409610 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 00/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 01/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 02/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 03/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 04/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 05/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 06/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 07/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 08/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 09/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 10/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 11/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 12/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 13/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 14/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 15/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 16/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 17/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 18/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 19/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 20/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 21/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 22/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 23/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 24/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 25/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 26/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 27/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 28/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 29/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 30/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Channel 31/32 : 0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Trees [0] -1/-1/-1-&gt;0-&gt;-1 [1] -1/-1/-1-&gt;0-&gt;-1 [2] -1/-1/-1-&gt;0-&gt;-1 [3] -1/-1/-1-&gt;0-&gt;-1 [4] -1/-1/-1-&gt;0-&gt;-1 [5] -1/-1/-1-&gt;0-&gt;-1 [6] -1/-1/-1-&gt;0-&gt;-1 [7] -1/-1/-1-&gt;0-&gt;-1 [8] -1/-1/-1-&gt;0-&gt;-1 [9] -1/-1/-1-&gt;0-&gt;-1 [10] -1/-1/-1-&gt;0-&gt;-1 [11] -1/-1/-1-&gt;0-&gt;-1 [12] -1/-1/-1-&gt;0-&gt;-1 [13] -1/-1/-1-&gt;0-&gt;-1 [14] -1/-1/-1-&gt;0-&gt;-1 [15] -1/-1/-1-&gt;0-&gt;-1 [16] -1/-1/-1-&gt;0-&gt;-1 [17] -1/-1/-1-&gt;0-&gt;-1 [18] -1/-1/-1-&gt;0-&gt;-1 [19] -1/-1/-1-&gt;0-&gt;-1 [20] -1/-1/-1-&gt;0-&gt;-1 [21] -1/-1/-1-&gt;0-&gt;-1 [22] -1/-1/-1-&gt;0-&gt;-1 [23] -1/-1/-1-&gt;0-&gt;-1 [24] -1/-1/-1-&gt;0-&gt;-1 [25] -1/-1/-1-&gt;0-&gt;-1 [26] -1/-1/-1-&gt;0-&gt;-1 [27] -1/-1/-1-&gt;0-&gt;-1 [28] -1/-1/-1-&gt;0-&gt;-1 [29] -1/-1/-1-&gt;0-&gt;-1 [30] -1/-1/-1-&gt;0-&gt;-1 [31] -1/-1/-1-&gt;0-&gt;-1</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO P2P Chunksize set to 131072</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58246 [0] NCCL INFO [Proxy Service] Device 0 CPU core 8</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58247 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58248 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 14</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so libnccl-net.so. Using internal tuner plugin.</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO ncclCommInitAll comm 0x559205409610 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1000 commId 0x89b784d33d72ebbb - Init COMPLETE</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58245 [0] NCCL INFO Init timings - ncclCommInitAll: rank 0 nranks 1 total 0.22 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.01, rest 0.00) </span></span>
<span class="line"><span>#</span></span>
<span class="line"><span>#                                                              out-of-place                       in-place</span></span>
<span class="line"><span>#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong</span></span>
<span class="line"><span>#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)</span></span>
<span class="line"><span>           8             2     float     sum      -1     9.32    0.00    0.00      0     0.14    0.06    0.00      0</span></span>
<span class="line"><span>          16             4     float     sum      -1    14.29    0.00    0.00      0     0.15    0.11    0.00      0</span></span>
<span class="line"><span>          32             8     float     sum      -1    12.24    0.00    0.00      0     0.15    0.22    0.00      0</span></span>
<span class="line"><span>          64            16     float     sum      -1    14.00    0.00    0.00      0     0.15    0.44    0.00      0</span></span>
<span class="line"><span>         128            32     float     sum      -1    11.98    0.01    0.00      0     1.06    0.12    0.00      0</span></span>
<span class="line"><span>         256            64     float     sum      -1    11.72    0.02    0.00      0     0.15    1.71    0.00      0</span></span>
<span class="line"><span>         512           128     float     sum      -1    13.87    0.04    0.00      0     0.15    3.53    0.00      0</span></span>
<span class="line"><span>        1024           256     float     sum      -1    13.81    0.07    0.00      0     0.15    6.83    0.00      0</span></span>
<span class="line"><span>        2048           512     float     sum      -1    17.77    0.12    0.00      0     0.15   13.65    0.00      0</span></span>
<span class="line"><span>        4096          1024     float     sum      -1    14.04    0.29    0.00      0     0.15   28.25    0.00      0</span></span>
<span class="line"><span>        8192          2048     float     sum      -1    13.54    0.61    0.00      0     0.15   54.61    0.00      0</span></span>
<span class="line"><span>       16384          4096     float     sum      -1    11.61    1.41    0.00      0     0.15  109.23    0.00      0</span></span>
<span class="line"><span>       32768          8192     float     sum      -1    13.64    2.40    0.00      0     0.15  218.45    0.00      0</span></span>
<span class="line"><span>       65536         16384     float     sum      -1    15.48    4.23    0.00      0     0.13  504.12    0.00      0</span></span>
<span class="line"><span>      131072         32768     float     sum      -1    23.09    5.68    0.00      0     0.13  1008.25    0.00      0</span></span>
<span class="line"><span>      262144         65536     float     sum      -1    25.09   10.45    0.00      0     0.14  1872.46    0.00      0</span></span>
<span class="line"><span>      524288        131072     float     sum      -1    26.29   19.94    0.00      0     0.15  3615.78    0.00      0</span></span>
<span class="line"><span>     1048576        262144     float     sum      -1    20.98   49.98    0.00      0     0.16  6553.60    0.00      0</span></span>
<span class="line"><span>     2097152        524288     float     sum      -1    26.15   80.20    0.00      0     0.16  13530.01    0.00      0</span></span>
<span class="line"><span>     4194304       1048576     float     sum      -1    37.97  110.46    0.00      0     0.15  27962.03    0.00      0</span></span>
<span class="line"><span>     8388608       2097152     float     sum      -1    66.97  125.27    0.00      0     0.14  59918.63    0.00      0</span></span>
<span class="line"><span>DESKTOP-P54EAF3:58239:58239 [0] NCCL INFO comm 0x559205409610 rank 0 nranks 1 cudaDev 0 busId 1000 - Destroy COMPLETE</span></span>
<span class="line"><span># Out of bounds values : 0 OK</span></span>
<span class="line"><span># Avg bus bandwidth    : 0</span></span>
<span class="line"><span>#</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="参数" tabindex="-1"><a class="header-anchor" href="#参数"><span>参数</span></a></h3><p>所有测试支持相同的一组参数：</p><ul><li>GPU 数量 <ul><li><code>-t,--nthreads &lt;线程数&gt;</code> 每个进程的线程数。默认值：1。</li><li><code>-g,--ngpus &lt;每个线程的 GPU 数量&gt;</code> 每个线程使用的 GPU 数量。默认值：1。</li></ul></li><li>扫描的大小范围 <ul><li><code>-b,--minbytes &lt;最小字节数&gt;</code> 开始的最小大小。默认值：32M。</li><li><code>-e,--maxbytes &lt;最大字节数&gt;</code> 结束的最大大小。默认值：32M。</li><li>递增方式可以是固定递增或乘法因子。仅应使用其中一种方式： <ul><li><code>-i,--stepbytes &lt;递增字节数&gt;</code> 各大小之间的固定增量。默认值：1M。</li><li><code>-f,--stepfactor &lt;递增因子&gt;</code> 各大小之间的乘法因子。默认值：禁用。</li></ul></li></ul></li><li>NCCL 操作参数 <ul><li><code>-o,--op &lt;sum/prod/min/max/avg/all&gt;</code> 指定执行哪种规约操作。仅对 Allreduce、Reduce 或 ReduceScatter 等规约操作有效。默认值：Sum（求和）。</li><li><code>-d,--datatype &lt;nccltype/all&gt;</code> 指定使用哪种数据类型。默认值：Float（浮点数）。</li><li><code>-r,--root &lt;root/all&gt;</code> 指定使用哪个 root，仅对有 root 的操作（如广播或规约）有效。默认值：0。</li></ul></li><li>性能参数 <ul><li><code>-n,--iters &lt;迭代次数&gt;</code> 迭代次数。默认值：20。</li><li><code>-w,--warmup_iters &lt;预热迭代次数&gt;</code> 预热迭代次数（不计时）。默认值：5。</li><li><code>-m,--agg_iters &lt;聚合次数&gt;</code> 每次迭代中聚合在一起的操作次数。默认值：1。</li><li><code>-N,--run_cycles &lt;循环次数&gt;</code> 运行并打印每个循环。默认值：1；0=无限循环。</li><li><code>-a,--average &lt;0/1/2/3&gt;</code> 将性能报告为所有节点的平均值（仅适用于 MPI=1）。&lt;0=Rank0, 1=平均, 2=最小, 3=最大&gt;。默认值：1（平均）。</li></ul></li><li>测试操作 <ul><li><code>-p,--parallel_init &lt;0/1&gt;</code> 使用线程并行初始化 NCCL。默认值：0。</li><li><code>-c,--check &lt;检查迭代次数&gt;</code> 执行指定次数的迭代，每次迭代都检查结果的正确性。对于大量 GPU 来说，这可能非常慢。默认值：1。</li><li><code>-z,--blocking &lt;0/1&gt;</code> 使 NCCL 操作阻塞，即在每次操作后等待并同步 CPU。默认值：0。</li><li><code>-G,--cudagraph &lt;CUDA 图启动次数&gt;</code> 将迭代捕获为 CUDA 图，然后重复指定次数。默认值：0。</li><li><code>-C,--report_cputime &lt;0/1&gt;</code> 报告 CPU 时间而非延迟。默认值：0。</li><li><code>-R,--local_register &lt;1/0&gt;</code> 在发送/接收缓冲区上启用本地缓冲区注册。默认值：0。</li><li><code>-T,--timeout &lt;超时时间（秒）&gt;</code> 在指定秒数后对每个测试进行超时。默认值：禁用。</li></ul></li></ul><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>def get_group_rank(group: ProcessGroup, global_rank: int) -&gt; int:</span></span>
<span class="line"><span>    &quot;&quot;&quot;</span></span>
<span class="line"><span>    Translate a global rank into a group rank.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    \`\`global_rank\`\` must be part of \`\`group\`\` otherwise this raises RuntimeError.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    Args:</span></span>
<span class="line"><span>        group (ProcessGroup): ProcessGroup to find the relative rank.</span></span>
<span class="line"><span>        global_rank (int): Global rank to query.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    Returns:</span></span>
<span class="line"><span>        Group rank of \`\`global_rank\`\` relative to \`\`group\`\`</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    N.B. calling this function on the default process group returns identity</span></span>
<span class="line"><span>    &quot;&quot;&quot;</span></span>
<span class="line"><span>    if group is GroupMember.WORLD:</span></span>
<span class="line"><span>        return global_rank</span></span>
<span class="line"><span>    if group not in _world.pg_group_ranks:</span></span>
<span class="line"><span>        raise ValueError(</span></span>
<span class="line"><span>            f&quot;Group {group} is not registered, please create group with torch.distributed.new_group API&quot;</span></span>
<span class="line"><span>        )</span></span>
<span class="line"><span>    group_ranks = _world.pg_group_ranks[group]</span></span>
<span class="line"><span>    if global_rank not in group_ranks:</span></span>
<span class="line"><span>        raise ValueError(f&quot;Global rank {global_rank} is not part of group {group}&quot;)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    return group_ranks[global_rank]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这段代码的主要功能是将一个全局的 rank 转换为在某个进程组中的 rank。它实现了在分布式训练中，通过全局 rank 查询某个特定进程组内的相对 rank。如果 <code>global_rank</code> 不在给定的进程组 <code>group</code> 中，该函数会抛出 <code>ValueError</code> 异常。</p><h3 id="具体工作原理" tabindex="-1"><a class="header-anchor" href="#具体工作原理"><span>具体工作原理</span></a></h3><ol><li><p><strong>判断是否为默认进程组</strong>：</p><ul><li>如果 <code>group</code> 是默认的进程组 <code>GroupMember.WORLD</code>，则直接返回 <code>global_rank</code>，因为默认进程组中的 rank 就是全局 rank，保持不变。</li></ul></li><li><p><strong>检查组是否注册</strong>：</p><ul><li>如果 <code>group</code> 没有在 <code>_world.pg_group_ranks</code> 中找到，则抛出异常。<code>_world.pg_group_ranks</code> 存储了所有创建的进程组及其对应的 ranks。</li></ul></li><li><p><strong>检查全局 rank 是否在该组中</strong>：</p><ul><li>如果 <code>global_rank</code> 不在 <code>group_ranks</code> 列表中，抛出异常。<code>group_ranks</code> 保存了 <code>group</code> 中的全局 rank 映射。</li></ul></li><li><p><strong>返回相对 rank</strong>：</p><ul><li>最终根据 <code>group_ranks</code> 返回 <code>global_rank</code> 在 <code>group</code> 中的相对 rank。</li></ul></li></ol><h3 id="参数说明" tabindex="-1"><a class="header-anchor" href="#参数说明"><span>参数说明：</span></a></h3><ul><li><code>group (ProcessGroup)</code>：PyTorch 分布式的进程组，通常由 <code>torch.distributed.new_group</code> 创建。</li><li><code>global_rank (int)</code>：全局 rank 值，通常是指分布式训练中全局进程的编号。</li></ul><h3 id="返回" tabindex="-1"><a class="header-anchor" href="#返回"><span>返回：</span></a></h3><ul><li>返回值是 <code>global_rank</code> 在指定 <code>group</code> 中的 rank。</li></ul><h3 id="代码解释" tabindex="-1"><a class="header-anchor" href="#代码解释"><span>代码解释：</span></a></h3><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> group </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">is</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> GroupMember.</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">WORLD</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> global_rank</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>这段代码检查传入的 <code>group</code> 是否是默认的全局进程组。如果是的话，<code>global_rank</code> 和相对 rank 是一致的，直接返回。</li></ul><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> group </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">not</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;"> in</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> _world.pg_group_ranks:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">    raise</span><span style="--shiki-light:#005CC5;--shiki-dark:#ABB2BF;"> ValueError</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">        f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;Group </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">group</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;"> is not registered, please create group with torch.distributed.new_group API&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    )</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>这里检查是否传入的 <code>group</code> 已经被注册到 <code>_world.pg_group_ranks</code> 中。如果没有注册，则抛出 <code>ValueError</code>，并提示用户需要通过 <code>torch.distributed.new_group</code> 来创建该组。</li></ul><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">group_ranks </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> _world.pg_group_ranks[group]</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> global_rank </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">not</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;"> in</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> group_ranks:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">    raise</span><span style="--shiki-light:#005CC5;--shiki-dark:#ABB2BF;"> ValueError</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;Global rank </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">global_rank</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;"> is not part of group </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">group</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>这段代码从 <code>_world.pg_group_ranks</code> 获取当前 <code>group</code> 的所有全局 rank。如果 <code>global_rank</code> 不在该组中，则抛出异常。</li></ul><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">return</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> group_ranks[global_rank]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>最终返回 <code>global_rank</code> 在 <code>group</code> 中的相对 rank。</li></ul><h3 id="应用场景" tabindex="-1"><a class="header-anchor" href="#应用场景"><span>应用场景</span></a></h3><p>此功能非常适用于在分布式训练中处理多个进程组的情况，用户可以轻松找到某个全局 rank 在特定组内的 rank，从而进行更细粒度的进程控制或通信。</p><h2 id="torch-cuda-nccl-py" tabindex="-1"><a class="header-anchor" href="#torch-cuda-nccl-py"><span>torch/cuda/nccl.py</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span># \`output\` used to be \`outputs\`, taking in a list of tensors. So we have two</span></span>
<span class="line"><span># arguments for BC reasons.</span></span>
<span class="line"><span>def reduce(</span></span>
<span class="line"><span>    inputs: Sequence[torch.Tensor],</span></span>
<span class="line"><span>    output: Optional[Union[torch.Tensor, Sequence[torch.Tensor]]] = None,</span></span>
<span class="line"><span>    root: int = 0,</span></span>
<span class="line"><span>    op: int = SUM,</span></span>
<span class="line"><span>    streams: Optional[Sequence[torch.cuda.Stream]] = None,</span></span>
<span class="line"><span>    comms=None,</span></span>
<span class="line"><span>    *,</span></span>
<span class="line"><span>    outputs: Optional[Sequence[torch.Tensor]] = None,</span></span>
<span class="line"><span>) -&gt; None:</span></span>
<span class="line"><span>    _check_sequence_type(inputs)</span></span>
<span class="line"><span>    _output: torch.Tensor</span></span>
<span class="line"><span>    if outputs is not None:</span></span>
<span class="line"><span>        if output is not None:</span></span>
<span class="line"><span>            raise ValueError(</span></span>
<span class="line"><span>                &quot;&#39;output&#39; and &#39;outputs&#39; can not be both specified. &#39;outputs&#39; is deprecated in &quot;</span></span>
<span class="line"><span>                &quot;favor of &#39;output&#39;, taking in a single output tensor. The signature of reduce is: &quot;</span></span>
<span class="line"><span>                &quot;reduce(inputs, output=None, root=0, op=SUM, streams=None, comms=None).&quot;</span></span>
<span class="line"><span>            )</span></span>
<span class="line"><span>        else:</span></span>
<span class="line"><span>            warnings.warn(</span></span>
<span class="line"><span>                &quot;\`nccl.reduce\` with an output tensor list is deprecated. &quot;</span></span>
<span class="line"><span>                &quot;Please specify a single output tensor with argument &#39;output&#39; instead instead.&quot;,</span></span>
<span class="line"><span>                FutureWarning,</span></span>
<span class="line"><span>                stacklevel=2,</span></span>
<span class="line"><span>            )</span></span>
<span class="line"><span>            _output = outputs[root]</span></span>
<span class="line"><span>    elif not isinstance(output, torch.Tensor) and isinstance(</span></span>
<span class="line"><span>        output, collections.abc.Sequence</span></span>
<span class="line"><span>    ):</span></span>
<span class="line"><span>        # User called old API with positional arguments of list of output tensors.</span></span>
<span class="line"><span>        warnings.warn(</span></span>
<span class="line"><span>            &quot;nccl.reduce with an output tensor list is deprecated. &quot;</span></span>
<span class="line"><span>            &quot;Please specify a single output tensor.&quot;,</span></span>
<span class="line"><span>            FutureWarning,</span></span>
<span class="line"><span>            stacklevel=2,</span></span>
<span class="line"><span>        )</span></span>
<span class="line"><span>        _output = output[root]</span></span>
<span class="line"><span>    else:</span></span>
<span class="line"><span>        _output = inputs[root] if output is None else output</span></span>
<span class="line"><span>    torch._C._nccl_reduce(inputs, _output, root, op, streams, comms)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这段代码实现了 <code>reduce</code> 函数的分布式操作，用于将多个 GPU 的张量根据某种操作（如求和）合并到一个目标张量中，特别是在使用 PyTorch 的 NCCL 后端时。这是常见的分布式通信操作，例如在多卡训练中汇总各个设备上的张量。</p><h3 id="核心功能" tabindex="-1"><a class="header-anchor" href="#核心功能"><span>核心功能</span></a></h3><p>该函数通过调用 <code>torch._C._nccl_reduce</code> 来实现具体的 <code>reduce</code> 操作，该函数执行 GPU 间张量的归约（如求和、乘积等操作），并将结果存储在某个 root 节点的目标张量中。</p><h3 id="参数解析" tabindex="-1"><a class="header-anchor" href="#参数解析"><span>参数解析</span></a></h3><ol><li><p><strong>inputs</strong>:</p><ul><li><code>Sequence[torch.Tensor]</code>，表示输入的张量序列，来自不同的 GPU。每个张量包含设备上局部计算的结果。</li></ul></li><li><p><strong>output</strong>:</p><ul><li><code>Optional[Union[torch.Tensor, Sequence[torch.Tensor]]]</code>，可选参数，用于存放归约操作的结果。如果没有提供，将使用 root 节点上的输入张量。</li></ul></li><li><p><strong>root</strong>:</p><ul><li><code>int</code>，表示哪个 GPU 作为 root，将接收所有 GPU 的归约结果。默认是 <code>0</code>。</li></ul></li><li><p><strong>op</strong>:</p><ul><li><code>int</code>，指定归约操作，默认为 <code>SUM</code>（加法）。其它可能的操作有 <code>PROD</code>（乘法）、<code>MIN</code>（最小值）、<code>MAX</code>（最大值）等。</li></ul></li><li><p><strong>streams</strong>:</p><ul><li><code>Optional[Sequence[torch.cuda.Stream]]</code>，可以为每个 GPU 提供 CUDA 流，方便在不同 CUDA 流上进行归约操作。默认为空，即使用默认流。</li></ul></li><li><p><strong>comms</strong>:</p><ul><li>可选的通信器对象，负责管理 GPU 间的通信。</li></ul></li><li><p><strong>outputs</strong>:</p><ul><li><code>Optional[Sequence[torch.Tensor]]</code>，这是一个过时参数，用于指定多个输出张量的列表。新的 API 只需要传入一个单一的 <code>output</code>，如果同时传入 <code>outputs</code> 和 <code>output</code>，则会报错。</li></ul></li></ol><h3 id="核心逻辑" tabindex="-1"><a class="header-anchor" href="#核心逻辑"><span>核心逻辑</span></a></h3><ol><li><p><strong>处理参数的兼容性</strong>:</p><ul><li>首先检查 <code>inputs</code> 的类型是否正确。接着处理参数 <code>outputs</code> 和 <code>output</code> 的兼容性，确保两者不会同时传入。如果用户使用了旧版 API (<code>outputs</code>)，会抛出警告，提醒用户该功能将被弃用。</li></ul></li><li><p><strong>处理旧 API</strong>:</p><ul><li>如果 <code>output</code> 是一个张量序列而不是单个张量，函数会继续支持这种旧的用法，但是同样会抛出警告，提示用户迁移到新的 API。</li></ul></li><li><p><strong>调用底层 NCCL 函数</strong>:</p><ul><li>最后，函数调用 <code>torch._C._nccl_reduce</code> 执行真正的张量归约操作，使用给定的 <code>inputs</code>、<code>_output</code>、<code>root</code>、<code>op</code> 和其他可选参数。</li></ul></li></ol><h3 id="示例" tabindex="-1"><a class="header-anchor" href="#示例"><span>示例</span></a></h3><p>假设我们在分布式训练中使用 4 个 GPU，执行 <code>reduce</code> 操作：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> [torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">tensor</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">([</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">]).</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">cuda</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(i) </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)]  </span><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 各个 GPU 上的张量</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">tensor</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">([</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">0.0</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">]).</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">cuda</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># root GPU 上的输出张量</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;">reduce</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(inputs, </span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">output</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">output, </span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">root</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(output)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 输出归约后的结果</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这段代码将各个 GPU 上的张量相加，并将结果存储在 root GPU（GPU 0）的 <code>output</code> 张量中。</p><h2 id="torch-csrc-cuda-python-nccl-cpp" tabindex="-1"><a class="header-anchor" href="#torch-csrc-cuda-python-nccl-cpp"><span>torch/csrc/cuda/python_nccl.cpp</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>PyObject* THCPModule_nccl_reduce(PyObject* self, PyObject* args) {</span></span>
<span class="line"><span>  HANDLE_TH_ERRORS</span></span>
<span class="line"><span>  PyObject *_inputs = nullptr, *_output = nullptr, *_streams = nullptr,</span></span>
<span class="line"><span>           *_comms = nullptr;</span></span>
<span class="line"><span>  int root = 0, op = 0;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  if (!PyArg_ParseTuple(</span></span>
<span class="line"><span>          args, &quot;OOiiOO&quot;, &amp;_inputs, &amp;_output, &amp;root, &amp;op, &amp;_streams, &amp;_comms)) {</span></span>
<span class="line"><span>    THPUtils_invalidArguments(</span></span>
<span class="line"><span>        args,</span></span>
<span class="line"><span>        nullptr,</span></span>
<span class="line"><span>        &quot;nccl_reduce&quot;,</span></span>
<span class="line"><span>        1,</span></span>
<span class="line"><span>        &quot;(sequence[Tensor] inputs, Tensor output, int root,&quot;</span></span>
<span class="line"><span>        &quot; int op, sequence[torch.cuda.Stream or None]&quot;);</span></span>
<span class="line"><span>    return nullptr;</span></span>
<span class="line"><span>  }</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  std::vector&lt;at::Tensor&gt; inputs = extract_tensors(_inputs);</span></span>
<span class="line"><span>  auto output = extract_tensor(_output);</span></span>
<span class="line"><span>  std::vector&lt;std::optional&lt;at::cuda::CUDAStream&gt;&gt; streams =</span></span>
<span class="line"><span>      unpack_streams(_streams, inputs.size());</span></span>
<span class="line"><span>  auto user_comms = unpack_comms(_comms, inputs.size());</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  {</span></span>
<span class="line"><span>    pybind11::gil_scoped_release no_gil;</span></span>
<span class="line"><span>    torch::cuda::nccl::reduce(inputs, output, root, op, streams, user_comms);</span></span>
<span class="line"><span>  }</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  Py_RETURN_NONE;</span></span>
<span class="line"><span>  END_HANDLE_TH_ERRORS</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>上述代码是 PyTorch 中一个 C++ 函数，它通过 Python 的 C API 实现了 NCCL <code>reduce</code> 操作的接口。这个函数可以在 Python 中调用，从而完成 NCCL 的 <code>reduce</code> 操作。NCCL（NVIDIA Collective Communications Library）是一种用于多 GPU 间高效通信的库，特别适用于深度学习框架中的集体通信操作，如 <code>reduce</code>、<code>allreduce</code> 等。</p><h3 id="代码解析" tabindex="-1"><a class="header-anchor" href="#代码解析"><span>代码解析</span></a></h3><ol><li><p><strong>函数定义</strong>:</p><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#E5C07B;">PyObject</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">*</span><span style="--shiki-light:#6F42C1;--shiki-dark:#61AFEF;"> THCPModule_nccl_reduce</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#E5C07B;">PyObject</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">*</span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#E5C07B;">PyObject</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">*</span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"> args</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这是一个 Python C API 的函数，<code>PyObject*</code> 是 Python 对象的通用类型，<code>self</code> 通常指模块对象，而 <code>args</code> 是从 Python 传递给这个函数的参数。</p></li><li><p><strong>参数解析</strong>:</p><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> (</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">!</span><span style="--shiki-light:#6F42C1;--shiki-dark:#61AFEF;">PyArg_ParseTuple</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">        args, </span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;OOiiOO&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">_inputs, </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">_output, </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">root, </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">op, </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">_streams, </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">_comms))</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p><code>PyArg_ParseTuple</code> 用于从 Python 参数 <code>args</code> 中解析出输入的值。格式化字符串 <code>&quot;OOiiOO&quot;</code> 表示该函数期望接收的参数类型依次是：</p><ul><li><code>O</code>: Python 对象（<code>_inputs</code>）</li><li><code>O</code>: Python 对象（<code>_output</code>）</li><li><code>i</code>: 整数（<code>root</code>，指定哪个 GPU 是 root 节点）</li><li><code>i</code>: 整数（<code>op</code>，指定 NCCL 操作类型，如 <code>SUM</code> 或 <code>MAX</code> 等）</li><li><code>O</code>: Python 对象（<code>_streams</code>）</li><li><code>O</code>: Python 对象（<code>_comms</code>，通信对象）</li></ul><p>如果参数无法解析，将返回 <code>nullptr</code>，并抛出错误。</p></li><li><p><strong>参数转换</strong>:</p><ul><li><code>extract_tensors(_inputs)</code> 和 <code>extract_tensor(_output)</code> 是从 Python 对象中提取张量的自定义函数。</li><li><code>unpack_streams(_streams, inputs.size())</code> 是将 <code>streams</code> 从 Python 对象中解包成 <code>CUDAStream</code> 对象。</li><li><code>unpack_comms(_comms, inputs.size())</code> 同样用于将通信器对象解包。</li></ul></li><li><p><strong>释放 GIL 锁</strong>:</p><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#ABB2BF;">pybind11</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">::gil_scoped_release no_gil;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><code>pybind11::gil_scoped_release</code> 用于释放 Python 的全局解释器锁（GIL），允许并行执行。因为 NCCL 操作是 GPU 通信操作，可能耗时较长，所以在执行这些操作前释放 GIL 是必要的。</p></li><li><p><strong>调用 NCCL <code>reduce</code></strong>:</p><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#E5C07B;">torch</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">::</span><span style="--shiki-light:#6F42C1;--shiki-dark:#E5C07B;">cuda</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">::</span><span style="--shiki-light:#6F42C1;--shiki-dark:#E5C07B;">nccl</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">::</span><span style="--shiki-light:#6F42C1;--shiki-dark:#61AFEF;">reduce</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(inputs, output, root, op, streams, user_comms);</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>调用了 PyTorch CUDA 模块中的 NCCL <code>reduce</code> 函数，将输入张量 <code>inputs</code> 通过 NCCL 通信操作聚合成 <code>output</code> 张量，<code>root</code> 指定了哪块 GPU 是主节点（负责聚合结果），<code>op</code> 指定了操作类型，如 <code>SUM</code>。</p></li><li><p><strong>返回 <code>None</code></strong>:</p><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">Py_RETURN_NONE;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>函数完成后，返回 <code>None</code>，表示没有返回值。</p></li><li><p><strong>错误处理</strong>:</p><ul><li><code>HANDLE_TH_ERRORS</code> 和 <code>END_HANDLE_TH_ERRORS</code> 是用于捕获和处理 C++ 异常的宏。如果在函数执行过程中发生异常，它们将自动捕捉并将错误报告给 Python 层。</li></ul></li></ol><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h3><p>此函数的作用是在 Python 层提供了 NCCL <code>reduce</code> 操作的接口，允许用户在 Python 中调用 NCCL 底层的 C++ 实现，以高效地在多 GPU 间进行通信和数据聚合。整个过程中，C++ 代码负责参数解析、执行 NCCL 操作并返回结果，而 Python 代码则以封装的方式调用这些底层实现。这种模式在深度学习框架中非常常见，尤其是在需要加速的分布式训练场景下。</p><h2 id="torch-csrc-cuda-module-cpp" tabindex="-1"><a class="header-anchor" href="#torch-csrc-cuda-module-cpp"><span>torch/csrc/cuda/Module.cpp</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>#ifdef USE_NCCL</span></span>
<span class="line"><span>    {&quot;_nccl_version&quot;, THCPModule_nccl_version, METH_NOARGS, nullptr},</span></span>
<span class="line"><span>    {&quot;_nccl_version_suffix&quot;,</span></span>
<span class="line"><span>     THCPModule_nccl_version_suffix,</span></span>
<span class="line"><span>     METH_NOARGS,</span></span>
<span class="line"><span>     nullptr},</span></span>
<span class="line"><span>    {&quot;_nccl_unique_id&quot;, THCPModule_nccl_unique_id, METH_NOARGS, nullptr},</span></span>
<span class="line"><span>    {&quot;_nccl_init_rank&quot;, THCPModule_nccl_init_rank, METH_VARARGS, nullptr},</span></span>
<span class="line"><span>    {&quot;_nccl_reduce&quot;, THCPModule_nccl_reduce, METH_VARARGS, nullptr},</span></span>
<span class="line"><span>    {&quot;_nccl_all_reduce&quot;, THCPModule_nccl_all_reduce, METH_VARARGS, nullptr},</span></span>
<span class="line"><span>    {&quot;_nccl_broadcast&quot;, THCPModule_nccl_broadcast, METH_VARARGS, nullptr},</span></span>
<span class="line"><span>    {&quot;_nccl_all_gather&quot;, THCPModule_nccl_all_gather, METH_VARARGS, nullptr},</span></span>
<span class="line"><span>    {&quot;_nccl_reduce_scatter&quot;,</span></span>
<span class="line"><span>     THCPModule_nccl_reduce_scatter,</span></span>
<span class="line"><span>     METH_VARARGS,</span></span>
<span class="line"><span>     nullptr},</span></span>
<span class="line"><span>#endif</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这段代码是 PyTorch 的 C++ 代码片段，主要用于注册与 NCCL（NVIDIA Collective Communications Library）相关的 Python API 函数。通过这些函数，用户可以在 Python 中调用 NCCL 的功能，如初始化 NCCL、执行集合操作等。下面是对代码的逐行分析：</p><h3 id="代码结构和功能" tabindex="-1"><a class="header-anchor" href="#代码结构和功能"><span>代码结构和功能</span></a></h3><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">#ifdef</span><span style="--shiki-light:#6F42C1;--shiki-dark:#61AFEF;"> USE_NCCL</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>这一行使用条件编译指令来检查是否启用了 NCCL 支持。如果编译时定义了 <code>USE_NCCL</code>，则编译器会包含以下代码块。</li></ul><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;_nccl_version&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, THCPModule_nccl_version, METH_NOARGS, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">nullptr</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">},</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>这一行注册了 <code>_nccl_version</code> 函数，该函数将返回 NCCL 的版本信息。<code>METH_NOARGS</code> 表示该函数不接受任何参数。</li></ul><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;_nccl_version_suffix&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, THCPModule_nccl_version_suffix, METH_NOARGS, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">nullptr</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">},</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>注册了 <code>_nccl_version_suffix</code> 函数，返回 NCCL 版本的后缀。</li></ul><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;_nccl_unique_id&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, THCPModule_nccl_unique_id, METH_NOARGS, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">nullptr</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">},</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>注册了 <code>_nccl_unique_id</code> 函数，用于生成 NCCL 的唯一标识符（Unique ID）。</li></ul><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;_nccl_init_rank&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, THCPModule_nccl_init_rank, METH_VARARGS, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">nullptr</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">},</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>注册了 <code>_nccl_init_rank</code> 函数，接受参数并用于初始化 NCCL 通信。</li></ul><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;_nccl_reduce&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, THCPModule_nccl_reduce, METH_VARARGS, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">nullptr</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">},</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>注册了 <code>_nccl_reduce</code> 函数，用于执行 NCCL 的 reduce 操作，接受一组输入张量和输出张量。</li></ul><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;_nccl_all_reduce&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, THCPModule_nccl_all_reduce, METH_VARARGS, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">nullptr</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">},</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>注册了 <code>_nccl_all_reduce</code> 函数，用于执行所有进程的 reduce 操作。</li></ul><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;_nccl_broadcast&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, THCPModule_nccl_broadcast, METH_VARARGS, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">nullptr</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">},</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>注册了 <code>_nccl_broadcast</code> 函数，用于执行广播操作，将数据从一个进程传递到所有其他进程。</li></ul><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;_nccl_all_gather&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, THCPModule_nccl_all_gather, METH_VARARGS, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">nullptr</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">},</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>注册了 <code>_nccl_all_gather</code> 函数，用于收集所有进程的数据到每个进程中。</li></ul><div class="language-cpp line-numbers-mode" data-highlighter="shiki" data-ext="cpp" data-title="cpp" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;_nccl_reduce_scatter&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, THCPModule_nccl_reduce_scatter, METH_VARARGS, </span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">nullptr</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">},</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>注册了 <code>_nccl_reduce_scatter</code> 函数，用于执行 reduce scatter 操作，将数据先进行 reduce，然后再分散到所有进程。</li></ul><h3 id="总结-1" tabindex="-1"><a class="header-anchor" href="#总结-1"><span>总结</span></a></h3><p>这段代码负责将 NCCL 的相关操作函数注册到 PyTorch 的 Python 接口中。这样用户在使用 PyTorch 时，可以方便地调用这些 NCCL 的操作来进行高效的多GPU并行计算。这种设计使得 PyTorch 的 CUDA 加速功能与 NCCL 库的集体通信能力紧密集成。</p><h2 id="torch-dynamo-trace-rules-py" tabindex="-1"><a class="header-anchor" href="#torch-dynamo-trace-rules-py"><span>torch_dynamo\\trace_rules.py</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>def _load_obj_from_str(fully_qualified_name):</span></span>
<span class="line"><span>    module, obj_name = fully_qualified_name.rsplit(&quot;.&quot;, maxsplit=1)</span></span>
<span class="line"><span>    return getattr(importlib.import_module(module), obj_name)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这个 Python 函数 <code>_load_obj_from_str</code> 用于根据一个完全限定的对象名称（通常是模块路径和对象名称的组合）动态地加载模块中的对象。下面是对该函数的逐行分析：</p><h3 id="代码解析-1" tabindex="-1"><a class="header-anchor" href="#代码解析-1"><span>代码解析</span></a></h3><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#61AFEF;"> _load_obj_from_str</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#24292E;--shiki-dark:#D19A66;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">fully_qualified_name</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">):</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>定义了一个名为 <code>_load_obj_from_str</code> 的函数，接受一个参数 <code>fully_qualified_name</code>，这是一个字符串，包含模块路径和对象名称，例如 <code>module.submodule.ClassName</code>。</li></ul><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">    module, obj_name </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> fully_qualified_name.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">rsplit</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">maxsplit</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>使用 <code>rsplit</code> 方法将 <code>fully_qualified_name</code> 从右侧分割成模块名和对象名。<code>maxsplit=1</code> 参数确保只分割一次，这样得到的 <code>module</code> 是模块路径，而 <code>obj_name</code> 是模块中的对象名称。</li></ul><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;"> getattr</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(importlib.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">import_module</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(module), obj_name)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>使用 <code>importlib.import_module</code> 动态导入指定的模块。</li><li>使用 <code>getattr</code> 从导入的模块中获取指定的对象，返回这个对象。</li></ul><h3 id="示例用法" tabindex="-1"><a class="header-anchor" href="#示例用法"><span>示例用法</span></a></h3><p>假设你有一个模块结构如下：</p><div class="language-plaintext line-numbers-mode" data-highlighter="shiki" data-ext="plaintext" data-title="plaintext" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>my_package/</span></span>
<span class="line"><span>    __init__.py</span></span>
<span class="line"><span>    my_module.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>my_module.py</code> 内容如下：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#E5C07B;"> MyClass</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#61AFEF;"> greet</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#24292E;--shiki-dark:#E5C07B;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">        return</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;"> &quot;Hello, world!&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>你可以通过以下方式加载 <code>MyClass</code>：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">fully_qualified_name </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;"> &quot;my_package.my_module.MyClass&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">MyClass </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;"> _load_obj_from_str</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(fully_qualified_name)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">instance </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;"> MyClass</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(instance.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">greet</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">())  </span><span style="--shiki-light:#6A737D;--shiki-dark:#7F848E;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;"># 输出: Hello, world!</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="总结-2" tabindex="-1"><a class="header-anchor" href="#总结-2"><span>总结</span></a></h3><p>这个函数的主要作用是实现动态加载模块中的对象，允许你在运行时根据字符串路径获取对象。这种方法在需要根据配置文件或用户输入来加载类或函数时非常有用。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>#include &lt;Python.h&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>extern PyObject* initModule(void);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>#ifndef _WIN32</span></span>
<span class="line"><span>#ifdef __cplusplus</span></span>
<span class="line"><span>extern &quot;C&quot;</span></span>
<span class="line"><span>#endif</span></span>
<span class="line"><span>__attribute__((visibility(&quot;default&quot;))) PyObject* PyInit__C(void);</span></span>
<span class="line"><span>#endif</span></span>
<span class="line"><span></span></span>
<span class="line"><span>PyMODINIT_FUNC PyInit__C(void)</span></span>
<span class="line"><span>{</span></span>
<span class="line"><span>  return initModule();</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><a href="https://zhuanlan.zhihu.com/p/647999983" target="_blank" rel="noopener noreferrer">python调用C/C++参考示例</a></p><p>这段 C/C++ 代码片段展示了一个用于初始化 Python 扩展模块的函数实现。在 Python 中，C 扩展模块需要实现一个模块初始化函数，它会在 Python 加载模块时被调用。这段代码的主要目的是将 <code>initModule</code> 函数作为 Python 模块的初始化函数，并通过兼容不同平台的方式导出。</p><h3 id="代码解析-2" tabindex="-1"><a class="header-anchor" href="#代码解析-2"><span>代码解析</span></a></h3><ol><li><p><strong><code>extern PyObject* initModule(void);</code></strong>:</p><ul><li>声明了一个外部的 C 函数 <code>initModule</code>，它返回一个 <code>PyObject*</code>，用于初始化模块。这个函数可能在别的地方定义，并负责创建和初始化模块对象。</li></ul></li><li><p><strong>平台相关宏控制</strong>:</p><ul><li><code>#ifndef _WIN32</code>：如果不是 Windows 平台，则会编译接下来的代码，这表明这个部分主要是为非 Windows 平台准备的。</li><li><code>#ifdef __cplusplus</code> 和 <code>extern &quot;C&quot;</code>：如果是在 C++ 中编译，这一部分会通过 <code>extern &quot;C&quot;</code> 指定 C 语言链接方式，确保 Python 模块能够以 C 语言的符号表被正确识别。</li><li><code>__attribute__((visibility(&quot;default&quot;)))</code>：用于控制符号的可见性。它保证了 <code>PyInit__C</code> 函数在共享库中对外可见（动态链接时需要）。</li></ul></li><li><p><strong><code>PyMODINIT_FUNC PyInit__C(void)</code></strong>:</p><ul><li><code>PyMODINIT_FUNC</code> 是一个宏，通常定义为 <code>extern &quot;C&quot;</code>，它用于标识 Python 扩展模块的初始化函数。</li><li>Python 期望扩展模块的初始化函数遵循特定的命名规范。在 Python 3 中，初始化函数的名称应为 <code>PyInit_&lt;module_name&gt;</code>，这里是 <code>PyInit__C</code>，意味着 Python 加载的模块名称为 <code>_C</code>。</li><li>这个函数返回一个 <code>PyObject*</code>，即模块对象，这个对象是通过调用 <code>initModule()</code> 获得的。</li></ul></li></ol><h3 id="典型应用" tabindex="-1"><a class="header-anchor" href="#典型应用"><span>典型应用</span></a></h3><p>这种模块初始化机制在 C 扩展模块中很常见，特别是在涉及到 Python 与 C/C++ 互操作时。<code>initModule()</code> 通常会调用一系列的 C API 函数来创建 Python 对象，并将它们绑定到模块的命名空间中。</p><h4 id="示例解释" tabindex="-1"><a class="header-anchor" href="#示例解释"><span>示例解释</span></a></h4><div class="language-c line-numbers-mode" data-highlighter="shiki" data-ext="c" data-title="c" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">PyMODINIT_FUNC </span><span style="--shiki-light:#6F42C1;--shiki-dark:#61AFEF;">PyInit__C</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">void</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">  return</span><span style="--shiki-light:#6F42C1;--shiki-dark:#61AFEF;"> initModule</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">();</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>当 Python 导入 <code>_C</code> 模块时，Python 解释器会调用 <code>PyInit__C()</code> 函数。这个函数又调用了 <code>initModule()</code>，返回初始化后的模块对象。</li><li>这个模块对象包含了所有需要导出的 C 函数和对象，使得这些函数能够被 Python 调用。</li></ul><h3 id="总结-3" tabindex="-1"><a class="header-anchor" href="#总结-3"><span>总结</span></a></h3><p>这段代码展示了如何在非 Windows 环境下创建一个 C 扩展模块的初始化函数。它将 <code>initModule</code> 作为模块的实际初始化逻辑，用于设置模块的内容，并通过 <code>PyInit__C</code> 函数将其导出给 Python 使用。</p><h2 id="nccl-topo" tabindex="-1"><a class="header-anchor" href="#nccl-topo"><span>nccl topo</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span>#define NCCL_TOPO_MAX_NODES 256</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Init search. Needs to be done before calling ncclTopoCompute</span></span>
<span class="line"><span>ncclResult_t ncclTopoSearchInit(struct ncclTopoSystem* system);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>#define NCCL_TOPO_PATTERN_BALANCED_TREE 1   // Spread NIC traffic between two GPUs (Tree parent + one child on first GPU, second child on second GPU)</span></span>
<span class="line"><span>#define NCCL_TOPO_PATTERN_SPLIT_TREE 2      // Spread NIC traffic between two GPUs (Tree parent on first GPU, tree children on the second GPU)</span></span>
<span class="line"><span>#define NCCL_TOPO_PATTERN_TREE 3            // All NIC traffic going to/from the same GPU</span></span>
<span class="line"><span>#define NCCL_TOPO_PATTERN_RING 4            // Ring</span></span>
<span class="line"><span>#define NCCL_TOPO_PATTERN_NVLS 5            // NVLS+SHARP and NVLS+Tree</span></span>
<span class="line"><span>#define NCCL_TOPO_PATTERN_COLLNET_DIRECT 6  // Collnet Direct</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>NCCL（NVIDIA Collective Communications Library）拓扑结构的定义和初始化。这些定义用于描述不同的通信模式以及节点间的互联方式，尤其是如何在GPU之间分配网络接口卡（NIC）流量的策略。</p><h3 id="代码解析-3" tabindex="-1"><a class="header-anchor" href="#代码解析-3"><span>代码解析：</span></a></h3><ol><li><p><strong><code>NCCL_TOPO_MAX_NODES 256</code></strong>： 这个宏定义了NCCL拓扑结构支持的<strong>最大节点数为256</strong>。这里的节点可以是GPU或网络设备（如NIC），意味着NCCL拓扑在设计上最多支持256个节点的互联。这与前面提到的NCCL理论上支持256卡互联是一致的。</p></li><li><p><strong><code>ncclTopoSearchInit(struct ncclTopoSystem* system)</code></strong>： 这个函数用于初始化NCCL拓扑搜索过程，准备计算通信拓扑。在调用 <code>ncclTopoCompute</code> 之前，必须先调用这个函数以初始化系统结构。</p></li><li><p><strong>拓扑通信模式定义</strong>： NCCL提供了多种通信模式（<code>TOPO_PATTERN</code>），这些模式定义了不同情况下网络流量如何在GPU之间分布：</p><ul><li><strong><code>NCCL_TOPO_PATTERN_BALANCED_TREE</code></strong> (1): 平衡树形结构，NIC流量分布在两个GPU之间。树的父节点和一个子节点在第一个GPU上，第二个子节点在第二个GPU上。</li><li><strong><code>NCCL_TOPO_PATTERN_SPLIT_TREE</code></strong> (2): 拆分树形结构，NIC流量分布在两个GPU之间。树的父节点在第一个GPU上，子节点都在第二个GPU上。</li><li><strong><code>NCCL_TOPO_PATTERN_TREE</code></strong> (3): 所有的NIC流量都集中在同一个GPU上。</li><li><strong><code>NCCL_TOPO_PATTERN_RING</code></strong> (4): 环形结构，流量在GPU之间按顺序传递。</li><li><strong><code>NCCL_TOPO_PATTERN_NVLS</code></strong> (5): 结合NVLS（NVIDIA Virtual Link Switch）和SHARP（Scalable Hierarchical Aggregation and Reduction Protocol）技术，进行通信优化。</li><li><strong><code>NCCL_TOPO_PATTERN_COLLNET_DIRECT</code></strong> (6): CollNet直接通信模式，通常用于优化大规模集群中的点对点通信。</li></ul></li></ol><h3 id="拓扑结构的意义" tabindex="-1"><a class="header-anchor" href="#拓扑结构的意义"><span>拓扑结构的意义：</span></a></h3><p>这些不同的拓扑模式允许NCCL根据具体的硬件拓扑和通信需求，选择最优的通信模式，以最大化带宽利用率、减少延迟，并优化多GPU、多节点的分布式训练任务。尤其在超大规模GPU集群中（例如256个GPU互联的场景），选择合适的拓扑模式对性能有显著影响。</p><p>因此，在实际使用中，理解这些拓扑模式的定义，并结合硬件架构和通信需求，能够有效提升NCCL的通信性能。</p>`,133),t=[l];function p(d,c){return n(),i("div",null,t)}const h=s(e,[["render",p],["__file","nccl.html.vue"]]),o=JSON.parse('{"path":"/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/nccl.html","title":"nccl","lang":"zh-CN","frontmatter":{"date":"2024-10-08T00:00:00.000Z","title":"nccl","category":["nccl"],"tag":["nccl"],"description":"nccl nccl NCCL是一款独立的库，提供标准的 GPU 通信例程，支持全规约（all-reduce）、全收集（all-gather）、规约（reduce）、广播（broadcast）、规约并散播（reduce-scatter）以及任意基于发送/接收的通信模式。该库经过优化，能够在使用 PCIe、NVLink、NVSwitch 以及基于 Infi...","head":[["meta",{"property":"og:url","content":"https://mousesun846.github.io/KnowledgeBlog/KnowledgeBlog/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/nccl.html"}],["meta",{"property":"og:site_name","content":"知识笔记"}],["meta",{"property":"og:title","content":"nccl"}],["meta",{"property":"og:description","content":"nccl nccl NCCL是一款独立的库，提供标准的 GPU 通信例程，支持全规约（all-reduce）、全收集（all-gather）、规约（reduce）、广播（broadcast）、规约并散播（reduce-scatter）以及任意基于发送/接收的通信模式。该库经过优化，能够在使用 PCIe、NVLink、NVSwitch 以及基于 Infi..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-10-09T06:04:33.000Z"}],["meta",{"property":"article:author","content":"MouseSun"}],["meta",{"property":"article:tag","content":"nccl"}],["meta",{"property":"article:published_time","content":"2024-10-08T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-10-09T06:04:33.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"nccl\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-10-08T00:00:00.000Z\\",\\"dateModified\\":\\"2024-10-09T06:04:33.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"MouseSun\\",\\"url\\":\\"https://github.com/MouseSun846\\",\\"email\\":\\"\\"}]}"]]},"headers":[{"level":2,"title":"nccl","slug":"nccl","link":"#nccl","children":[{"level":3,"title":"下载源码","slug":"下载源码","link":"#下载源码","children":[]},{"level":3,"title":"构建编译","slug":"构建编译","link":"#构建编译","children":[]}]},{"level":2,"title":"nccl-test","slug":"nccl-test","link":"#nccl-test","children":[{"level":3,"title":"下载","slug":"下载","link":"#下载","children":[]},{"level":3,"title":"编译","slug":"编译","link":"#编译","children":[]},{"level":3,"title":"运行","slug":"运行","link":"#运行","children":[]},{"level":3,"title":"参数","slug":"参数","link":"#参数","children":[]},{"level":3,"title":"具体工作原理","slug":"具体工作原理","link":"#具体工作原理","children":[]},{"level":3,"title":"参数说明：","slug":"参数说明","link":"#参数说明","children":[]},{"level":3,"title":"返回：","slug":"返回","link":"#返回","children":[]},{"level":3,"title":"代码解释：","slug":"代码解释","link":"#代码解释","children":[]},{"level":3,"title":"应用场景","slug":"应用场景","link":"#应用场景","children":[]}]},{"level":2,"title":"torch/cuda/nccl.py","slug":"torch-cuda-nccl-py","link":"#torch-cuda-nccl-py","children":[{"level":3,"title":"核心功能","slug":"核心功能","link":"#核心功能","children":[]},{"level":3,"title":"参数解析","slug":"参数解析","link":"#参数解析","children":[]},{"level":3,"title":"核心逻辑","slug":"核心逻辑","link":"#核心逻辑","children":[]},{"level":3,"title":"示例","slug":"示例","link":"#示例","children":[]}]},{"level":2,"title":"torch/csrc/cuda/python_nccl.cpp","slug":"torch-csrc-cuda-python-nccl-cpp","link":"#torch-csrc-cuda-python-nccl-cpp","children":[{"level":3,"title":"代码解析","slug":"代码解析","link":"#代码解析","children":[]},{"level":3,"title":"总结","slug":"总结","link":"#总结","children":[]}]},{"level":2,"title":"torch/csrc/cuda/Module.cpp","slug":"torch-csrc-cuda-module-cpp","link":"#torch-csrc-cuda-module-cpp","children":[{"level":3,"title":"代码结构和功能","slug":"代码结构和功能","link":"#代码结构和功能","children":[]},{"level":3,"title":"总结","slug":"总结-1","link":"#总结-1","children":[]}]},{"level":2,"title":"torch_dynamo\\\\trace_rules.py","slug":"torch-dynamo-trace-rules-py","link":"#torch-dynamo-trace-rules-py","children":[{"level":3,"title":"代码解析","slug":"代码解析-1","link":"#代码解析-1","children":[]},{"level":3,"title":"示例用法","slug":"示例用法","link":"#示例用法","children":[]},{"level":3,"title":"总结","slug":"总结-2","link":"#总结-2","children":[]},{"level":3,"title":"代码解析","slug":"代码解析-2","link":"#代码解析-2","children":[]},{"level":3,"title":"典型应用","slug":"典型应用","link":"#典型应用","children":[]},{"level":3,"title":"总结","slug":"总结-3","link":"#总结-3","children":[]}]},{"level":2,"title":"nccl topo","slug":"nccl-topo","link":"#nccl-topo","children":[{"level":3,"title":"代码解析：","slug":"代码解析-3","link":"#代码解析-3","children":[]},{"level":3,"title":"拓扑结构的意义：","slug":"拓扑结构的意义","link":"#拓扑结构的意义","children":[]}]}],"git":{"createdTime":1728378516000,"updatedTime":1728453873000,"contributors":[{"name":"mousesun","email":"3026098675@qq.com","commits":5}]},"readingTime":{"minutes":24.23,"words":7269},"filePathRelative":"技术科普/nccl.md","localizedDate":"2024年10月8日","excerpt":"<div class=\\"hint-container tip\\">\\n<p class=\\"hint-container-title\\">nccl</p>\\n</div>\\n<h2>nccl</h2>\\n<p>NCCL是一款独立的库，提供标准的 GPU 通信例程，支持全规约（all-reduce）、全收集（all-gather）、规约（reduce）、广播（broadcast）、规约并散播（reduce-scatter）以及任意基于发送/接收的通信模式。该库经过优化，能够在使用 PCIe、NVLink、NVSwitch 以及基于 InfiniBand Verbs 或 TCP/IP 套接字的网络平台上实现高带宽。NCCL 支持任意数量的 GPU，无论是安装在单节点还是跨多个节点的系统中，并且可以在单进程或多进程（如 MPI）应用程序中使用。</p>","autoDesc":true}');export{h as comp,o as data};
