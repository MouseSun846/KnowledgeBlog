export const redirects = JSON.parse("{}")

export const routes = Object.fromEntries([
  ["/", { loader: () => import(/* webpackChunkName: "index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/index.html.js"), meta: {"t":"主页","i":"home"} }],
  ["/%E4%BA%91%E5%8E%9F%E7%94%9F/etcd.html", { loader: () => import(/* webpackChunkName: "云原生_etcd.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/云原生/etcd.html.js"), meta: {"d":1720396800000,"l":"2024年7月8日","c":["etcd"],"g":["分布式锁"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">etcd获取分布式锁</p>\n<div class=\"language- line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"\" data-title=\"\" style=\"--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34\"><pre class=\"shiki shiki-themes github-light one-dark-pro vp-code\"><code><span class=\"line\"><span>cli, err := clientv3.New(clientv3.Config{Endpoints: endpoints})</span></span>\n<span class=\"line\"><span>if err != nil {</span></span>\n<span class=\"line\"><span>   log.Fatal(err)</span></span>\n<span class=\"line\"><span>}</span></span>\n<span class=\"line\"><span>defer cli.Close()</span></span>\n<span class=\"line\"><span>// create two separate sessions for lock competition</span></span>\n<span class=\"line\"><span>s1, err := concurrency.NewSession(cli, concurrency.WithTTL(10))</span></span>\n<span class=\"line\"><span>if err != nil {</span></span>\n<span class=\"line\"><span>   log.Fatal(err)</span></span>\n<span class=\"line\"><span>}</span></span>\n<span class=\"line\"><span>defer s1.Close()</span></span>\n<span class=\"line\"><span>m1 := concurrency.NewMutex(s1, \"/my-lock/\")</span></span>\n<span class=\"line\"><span>// acquire lock for s1</span></span>\n<span class=\"line\"><span>if err := m1.Lock(context.TODO()); err != nil {</span></span>\n<span class=\"line\"><span>   log.Fatal(err)</span></span>\n<span class=\"line\"><span>}</span></span>\n<span class=\"line\"><span>fmt.Println(\"acquired lock for s1\")</span></span>\n<span class=\"line\"><span>if err := m1.Unlock(context.TODO()); err != nil {</span></span>\n<span class=\"line\"><span>   log.Fatal(err)</span></span>\n<span class=\"line\"><span>}</span></span>\n<span class=\"line\"><span>fmt.Println(\"released lock for s1\")</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>1、首先通过 concurrency.NewSession 方法创建 Session，本质是创建了一个 TTL 为 10 的 Lease。</p>\n<p>2、其次得到 session 对象后，通过 concurrency.NewMutex 创建了一个 mutex 对象，包含 Lease、key prefix 等信息。</p>\n<p>3、然后通过 mutex 对象的 Lock 方法尝试获取锁。</p>\n<p>当 CreateRevision 为 0 时，它会创建一个 prefix 为 /my-lock 的 key（ /my-lock + LeaseID)，并获取到 /my-lock prefix 下面最早创建的一个 key（revision 最小），分布式锁最终是由写入此 key 的 client 获得，其他 client 则进入等待模式。</p>\n<p>4、最后使用结束，可通过 mutex 对象的 Unlock 方法释放锁。</p>\n</div>","r":{"minutes":2.06,"words":619},"t":"etcd","y":"a"} }],
  ["/%E4%BA%91%E5%8E%9F%E7%94%9F/k8s.html", { loader: () => import(/* webpackChunkName: "云原生_k8s.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/云原生/k8s.html.js"), meta: {"d":1719964800000,"l":"2024年7月3日","c":["k8s"],"g":["笔记"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">k8s 笔记总结</p>\n</div>\n<h1>pv</h1>\n<h2>访问模式（Access Modes）</h2>\n<p>Kubernetes支持的访问模式如下。</p>\n<ul>\n<li>\n<p>ReadWriteOnce（RWO）：读写权限，并且只能被单个Node挂\n载。</p>\n</li>\n<li>\n<p>ReadOnlyMany（ROX）：只读权限，允许被多个Node挂载。</p>\n</li>\n<li>\n<p>ReadWriteMany（RWX）：读写权限，允许被多个Node挂载。</p>\n</li>\n</ul>","r":{"minutes":19.91,"words":5973},"t":"k8s知识点","y":"a"} }],
  ["/%E4%BA%91%E5%8E%9F%E7%94%9F/kong.html", { loader: () => import(/* webpackChunkName: "云原生_kong.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/云原生/kong.html.js"), meta: {"d":1725321600000,"l":"2024年9月3日","c":["kong"],"g":["kong","nginx","html"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">Kong和Nginx创建服务和路由</p>\n</div>\n<p>下面是你使用Kong和Nginx创建服务和路由的步骤总结：</p>\n<h3>1. Nginx容器启动</h3>\n<p>你通过以下命令启动了一个Nginx容器，该容器暴露8088端口并挂载了配置文件和项目目录：</p>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" data-title=\"bash\" style=\"--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34\"><pre class=\"shiki shiki-themes github-light one-dark-pro vp-code\"><code><span class=\"line\"><span style=\"--shiki-light:#6F42C1;--shiki-dark:#61AFEF\">docker</span><span style=\"--shiki-light:#032F62;--shiki-dark:#98C379\"> run</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> -itd</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> --name</span><span style=\"--shiki-light:#032F62;--shiki-dark:#98C379\"> nginx</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> --privileged</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> --restart=always</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> --network=kong-net</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> -m</span><span style=\"--shiki-light:#032F62;--shiki-dark:#98C379\"> 2GB</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> -p</span><span style=\"--shiki-light:#032F62;--shiki-dark:#98C379\"> 8088:8088</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> -v</span><span style=\"--shiki-light:#032F62;--shiki-dark:#98C379\"> /mnt/d/docker/nginx/project:/data/project</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> -v</span><span style=\"--shiki-light:#032F62;--shiki-dark:#98C379\"> /mnt/d/docker/nginx/conf/nginx.conf:/etc/nginx/nginx.conf</span><span style=\"--shiki-light:#032F62;--shiki-dark:#98C379\"> nginx:latest</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div>","r":{"minutes":3.4,"words":1019},"t":"Kong和Nginx部署服务","y":"a"} }],
  ["/%E4%BA%91%E5%8E%9F%E7%94%9F/kubeflow.html", { loader: () => import(/* webpackChunkName: "云原生_kubeflow.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/云原生/kubeflow.html.js"), meta: {"d":1717632000000,"l":"2024年6月6日","c":["云原生"],"g":["Kubeflow"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">k8s 云原生之Kubeflow 简介</p>\n</div>\n<h1>官网</h1>\n<p><a href=\"https://www.kubeflow.org/docs/started/introduction/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.kubeflow.org/docs/started/introduction/</a></p>\n<h1>介绍</h1>\n<p>Kubeflow 简介\nKubeflow 项目致力于让机器学习 (ML) 工作流在 Kubernetes 上的部署变得简单、可移植且可扩展。我们的目标不是重新创建其他服务，而是提供一种简单的方法，将最佳的 ML 开源系统部署到各种基础设施中。只要您运行 Kubernetes，就可以运行 Kubeflow。\n下图展示了主要的 Kubeflow 组件，涵盖 Kubernetes 之上 ML 生命周期的每个步骤。\n<img src=\"/assets/images/kubeflow-intro-diagram.drawio.svg\" alt=\"\" loading=\"lazy\"></p>","r":{"minutes":18.44,"words":5533},"t":"Kubeflow 简介","y":"a"} }],
  ["/%E4%BA%91%E5%8E%9F%E7%94%9F/operator.html", { loader: () => import(/* webpackChunkName: "云原生_operator.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/云原生/operator.html.js"), meta: {"d":1719532800000,"l":"2024年6月28日","c":["云原生"],"g":["operator"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">什么是 Kubernetes Operator？</p>\n</div>\n<p><strong>Kubernetes Operator</strong> 是一种软件扩展，使用 Kubernetes 原生的 API 和工具来自动管理复杂应用的生命周期。Operator 可以将人类操作员（例如系统管理员）的操作自动化，管理 Kubernetes 应用程序的配置、部署、升级、备份和故障处理等任务。</p>\n<h3>Operator 的基本概念</h3>\n<ul>\n<li>\n<p><strong>Custom Resource (自定义资源，CR)</strong>:\nKubernetes 的内置资源（如 Pod、Service）可能无法满足所有应用的需求。CR 提供了定义自定义对象的能力，使得用户可以在 Kubernetes 中引入新的资源类型。</p>\n</li>\n<li>\n<p><strong>Custom Resource Definition (自定义资源定义，CRD)</strong>:\nCRD 是 Kubernetes 用于定义 CR 结构的机制。通过 CRD，用户可以创建和管理新的自定义资源。</p>\n</li>\n<li>\n<p><strong>Controller (控制器)</strong>:\n控制器是 Kubernetes 中一个不断循环检查资源实际状态并使其符合预期状态的逻辑组件。Operator 就是一个高级的控制器，专门用于管理自定义资源。</p>\n</li>\n</ul>","r":{"minutes":3.57,"words":1072},"t":"k8s operator","y":"a"} }],
  ["/%E4%BB%8B%E7%BB%8D/%E4%BB%8B%E7%BB%8D.html", { loader: () => import(/* webpackChunkName: "介绍_介绍.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/介绍/介绍.html.js"), meta: {"d":1717632000000,"l":"2024年6月6日","c":["个人介绍"],"g":["个人"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">个人简介</p>\n<p>大家好，我是一名充满热情的全栈开发工程师，具备广泛的编程语言知识和丰富的实际项目经验。我始终追求技术创新，致力于通过高效、优质的代码解决复杂的业务问题。在开发过程中，我不仅关注性能优化和可扩展性，还注重代码的可维护性和团队协作，能够快速适应不同的项目需求。</p>\n</div>\n<h3>编程语言</h3>\n<ul>\n<li><strong>Java</strong>: 我在Java领域有着深厚的积累，尤其擅长使用Spring Boot框架开发企业级应用，能够设计并实现高并发、高性能的后端服务，保障系统的稳定性与可扩展性。</li>\n<li><strong>C++</strong>: 扎实的C++功底让我在系统级应用开发中游刃有余，能够编写高效、稳定的底层代码，解决复杂的系统问题。</li>\n<li><strong>C</strong>: 精通C语言，擅长底层系统开发，具备处理硬件交互及操作系统底层模块的经验。</li>\n<li><strong>Python</strong>: 熟练掌握Python，尤其在数据分析、机器学习领域有丰富经验，能够快速构建高效的分析和自动化工具。</li>\n<li><strong>Go</strong>: 熟练使用Go语言开发高并发应用，善于利用其高效的内存管理机制和原生的协程支持，构建稳定的后端服务。</li>\n</ul>","r":{"minutes":3.19,"words":958},"t":"个人介绍","y":"a"} }],
  ["/%E5%B7%A5%E5%85%B7/Clash%E8%BD%ACV2ray.html", { loader: () => import(/* webpackChunkName: "工具_Clash转V2ray.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/工具/Clash转V2ray.html.js"), meta: {"d":1724716800000,"l":"2024年8月27日","c":["代理"],"g":["clash","v2ray"],"r":{"minutes":0.06,"words":19},"t":"clash节点转为V2ray","y":"a"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2.html", { loader: () => import(/* webpackChunkName: "技术科普_AI大模型部署.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/AI大模型部署.html.js"), meta: {"d":1724716800000,"l":"2024年8月27日","c":["AIGC"],"g":["FastGpt","chatgpt-on-web"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">FastGpt</p>\n</div>\n<h2>地址</h2>\n<p><a href=\"https://github.com/labring/FastGPT\" target=\"_blank\" rel=\"noopener noreferrer\">https://github.com/labring/FastGPT</a></p>\n<h2>文档</h2>\n<p><a href=\"https://doc.tryfastgpt.ai/docs/\" target=\"_blank\" rel=\"noopener noreferrer\">https://doc.tryfastgpt.ai/docs/</a></p>","r":{"minutes":6.48,"words":1945},"t":"FastGpt+chatgpt-on-web","y":"a"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/pytorch.html", { loader: () => import(/* webpackChunkName: "技术科普_pytorch.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/pytorch.html.js"), meta: {"d":1725408000000,"l":"2024年9月4日","c":["分布式"],"g":["PyTorch","c10d"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">PyTorch c10d</p>\n</div>\n<p>PyTorch 的 <code>c10d</code> 通信库是用于分布式计算的核心组件，特别是在需要跨多个设备（如多个GPU或多台机器）进行并行计算时。<code>c10d</code> 提供了分布式数据并行（Distributed Data Parallel, DDP）的底层实现，支持高效的数据同步和通信操作。</p>\n<h3>主要功能</h3>\n<ol>\n<li>\n<p><strong>通信后端</strong>：\n<code>c10d</code> 支持多种通信后端（Backend），如：</p>\n<ul>\n<li><code>NCCL</code>: 适用于GPU间通信，特别是在NVIDIA硬件上。</li>\n<li><code>GLOO</code>: 适用于CPU和GPU的跨平台通信。</li>\n<li><code>MPI</code>: 基于Message Passing Interface，适合大规模分布式系统。</li>\n</ul>\n</li>\n<li>\n<p><strong>进程组（Process Group）</strong>：\n<code>c10d</code> 中的进程组是通信的基本单元，可以将多个进程组织成一个组，以便它们之间进行通信。可以在不同的进程组之间进行广播、归约、全归约等操作。</p>\n</li>\n<li>\n<p><strong>广播和同步</strong>：\n<code>c10d</code> 提供了对数据进行广播和同步的接口，确保在多个进程或设备之间一致地传递数据。例如，在多GPU训练中，同步不同GPU上的模型参数。</p>\n</li>\n<li>\n<p><strong>梯度同步</strong>：\n在分布式数据并行训练中，<code>c10d</code> 自动同步各个设备计算出的梯度，从而确保在所有设备上更新后的模型参数保持一致。</p>\n</li>\n<li>\n<p><strong>AllReduce操作</strong>：\n<code>c10d</code> 支持AllReduce操作，这是在分布式训练中非常常用的操作，能够高效地合并不同设备上的梯度并更新模型参数。</p>\n</li>\n</ol>","r":{"minutes":12.15,"words":3645},"t":"PyTorch","y":"a"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99.html", { loader: () => import(/* webpackChunkName: "技术科普_学习资料.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/学习资料.html.js"), meta: {"d":1727136000000,"l":"2024年9月24日","c":["资料"],"g":["学习资料"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">学习资料</p>\n<h2><a class=\"header-anchor\" href=\"#awesome-compression\"><span></span></a><a href=\"https://github.com/datawhalechina/awesome-compression\" target=\"_blank\" rel=\"noopener noreferrer\">Awesome Compression</a></h2>\n<h3>项目简介</h3>\n<p>随着ChatGPT的出圈，大语言模型层出不穷，并展现出非凡的能力，可以有效地解决各种问题。然而，这些模型通常需要大量的计算资源和内存，导致运行时资源消耗较高，限制了其在某些场景下的应用，让很多研究者望而却步。本项目使用通俗易懂的语言介绍模型的剪枝、量化、知识蒸馏等压缩方法，让更多的小白能更快了解到模型压缩技术。</p>\n<p>在线阅读地址: https://datawhalechina.github.io/awesome-compression</p>\n<h2><a class=\"header-anchor\" href=\"#unlock-huggingface\"><span></span></a><a href=\"https://github.com/datawhalechina/unlock-hf\" target=\"_blank\" rel=\"noopener noreferrer\">Unlock-HuggingFace</a></h2>\n<p>近年来，自然语言处理（NLP）领域随着 Transformer 架构的出现取得了突破性进展，HuggingFace 作为 NLP 社区的重要力量，提供了海量的预训练模型和众多强大易用的函数库，极大地降低了 NLP 应用开发的门槛。</p>\n<p>本项目旨在为学习者提供深入学习 HuggingFace😊 生态系统的教程，并通过完成生动有趣的具体项目提升学习者实践水平。</p>\n<p>unlock-hf在线阅读链接</p>\n<h2><a class=\"header-anchor\" href=\"#llm-deploy\"><span></span></a><a href=\"https://github.com/datawhalechina/llm-deploy\" target=\"_blank\" rel=\"noopener noreferrer\">LLM-Deploy</a></h2>\n<p>本教程主要侧重于模型/LLM推理和部署理论与实践，旨在成为你掌握LLM推理与部署艺术的伙伴，无论你是初涉此领域的新人，还是寻求深化专业技能的资深人士，都能在此找到通往成功部署大型语言模型的关键路径。</p>\n<h2><a class=\"header-anchor\" href=\"#self-llm\"><span></span></a><a href=\"https://github.com/datawhalechina/self-llm\" target=\"_blank\" rel=\"noopener noreferrer\">self-llm</a></h2>\n<p>本项目是一个围绕开源大模型、针对国内初学者、基于 Linux 平台的中国宝宝专属大模型教程，针对各类开源大模型提供包括环境配置、本地部署、高效微调等技能在内的全流程指导，简化开源大模型的部署、使用和应用流程，让更多的普通学生、研究者更好地使用开源大模型，帮助开源、自由的大模型更快融入到普通学习者的生活中。</p>\n<p>本项目的主要内容包括：</p>\n<p>基于 Linux 平台的开源 LLM 环境配置指南，针对不同模型要求提供不同的详细环境配置步骤；\n针对国内外主流开源 LLM 的部署使用教程，包括 LLaMA、ChatGLM、InternLM 等；\n开源 LLM 的部署应用指导，包括命令行调用、在线 Demo 部署、LangChain 框架集成等；\n开源 LLM 的全量微调、高效微调方法，包括分布式全量微调、LoRA、ptuning 等。\n  项目的主要内容就是教程，让更多的学生和未来的从业者了解和熟悉开源大模型的食用方法！任何人都可以提出issue或是提交PR，共同构建维护这个项目。</p>\n<h2><a class=\"header-anchor\" href=\"#llm-universe\"><span></span></a><a href=\"https://github.com/datawhalechina/llm-universe\" target=\"_blank\" rel=\"noopener noreferrer\">llm-universe</a></h2>\n<h3>项目简介</h3>\n<p>本项目是一个面向小白开发者的大模型应用开发教程，旨在基于阿里云服务器，结合个人知识库助手项目，通过一个课程完成大模型开发的重点入门，主要内容包括：</p>\n<h3>大模型简介，何为大模型、大模型特点是什么、LangChain 是什么，如何开发一个 LLM 应用，针对小白开发者的简单介绍；</h3>\n<p>如何调用大模型 API，本节介绍了国内外知名大模型产品 API 的多种调用方式，包括调用原生 API、封装为 LangChain LLM、封装为 Fastapi 等调用方式，同时将包括百度文心、讯飞星火、智谱AI等多种大模型 API 进行了统一形式封装；\n知识库搭建，不同类型知识库文档的加载、处理，向量数据库的搭建；\n构建 RAG 应用，包括将 LLM 接入到 LangChain 构建检索问答链，使用 Streamlit 进行应用部署\n验证迭代，大模型开发如何实现验证迭代，一般的评估方法有什么；\n本项目主要包括三部分内容：</p>\n<p>LLM 开发入门。V1 版本的简化版，旨在帮助初学者最快、最便捷地入门 LLM 开发，理解 LLM 开发的一般流程，可以搭建出一个简单的 Demo。\nLLM 开发技巧。LLM 开发更进阶的技巧，包括但不限于：Prompt Engineering、多类型源数据的处理、优化检索、召回精排、Agent 框架等\nLLM 应用实例。引入一些成功的开源案例，从本课程的角度出发，解析这些应用范例的 Idea、核心思路、实现框架，帮助初学者明白其可以通过 LLM 开发什么样的应用。</p>\n<h2><a class=\"header-anchor\" href=\"#tiny-universe\"><span></span></a><a href=\"https://github.com/datawhalechina/tiny-universe\" target=\"_blank\" rel=\"noopener noreferrer\">tiny-universe</a></h2>\n<p>本项目是一个从原理出发、以“白盒”为导向、围绕大模型全链路的“手搓”大模型指南，旨在帮助有传统深度学习基础的读者从底层原理出发，“纯手搓”搭建一个清晰、可用的大模型系统，包括大模型本身、RAG 框架、Agent 系统及大模型评估体系。本项目将从基础原理出发，深入剖析每一个技术点并附以完整的代码实现，以细致讲解和代码注释帮助读者独立复现大模型核心部分，并在复现中实现对大模型的深入理解与掌握。</p>\n<p>本项目旨在为广大学习者搭建一个清晰的、可用的、可复现的大模型世界，帮助每一位有兴趣的学习者纯手工独立搭建自己的 Tiny LLM Universe。</p>\n<p>本项目的主要内容包括：</p>\n<p>深入剖析大模型原理——Qwen Blog\n逐步预训练一个手搓大模型——Tiny Llama3\n如何评估你的大模型——Tiny Eval\n纯手工搭建 RAG 框架——Tiny RAG\n手搓一个最小的 Agent 系统——Tiny Agent\n深入理解大模型基础——Tiny Transformer</p>\n<h2><a class=\"header-anchor\" href=\"#llm-cookbook\"><span></span></a><a href=\"https://github.com/datawhalechina/llm-cookbook\" target=\"_blank\" rel=\"noopener noreferrer\">llm-cookbook</a></h2>\n<p>面向开发者的大模型手册 - LLM Cookbook</p>\n<h3>项目简介</h3>\n<p>本项目是一个面向开发者的大模型手册，针对国内开发者的实际需求，主打 LLM 全方位入门实践。本项目基于吴恩达老师大模型系列课程内容，对原课程内容进行筛选、翻译、复现和调优，覆盖从 Prompt Engineering 到 RAG 开发、模型微调的全部流程，用最适合国内学习者的方式，指导国内开发者如何学习、入门 LLM 相关项目。</p>\n<p>针对不同内容的特点，我们对共计 11 门吴恩达老师的大模型课程进行了翻译复现，并结合国内学习者的实际情况，对不同课程进行了分级和排序，初学者可以先系统学习我们的必修类课程，掌握入门 LLM 所有方向都需要掌握的基础技能和概念，再选择性地学习我们的选修类课程，在自己感兴趣的方向上不断探索和学习。</p>\n<p>如果有你非常喜欢但我们还没有进行复现的吴恩达老师大模型课程，我们欢迎每一位开发者参考我们已有课程的格式和写法来对课程进行复现并提交 PR，在 PR 审核通过后，我们会根据课程内容将课程进行分级合并。欢迎每一位开发者的贡献！</p>\n<p>在线阅读地址：面向开发者的 LLM 入门课程-在线阅读</p>\n<p>PDF下载地址：面向开发者的 LLM 入门教程-PDF</p>\n<p>英文原版地址：吴恩达关于大模型的系列课程</p>\n<h2><a class=\"header-anchor\" href=\"#handy-ollama\"><span></span></a><a href=\"https://github.com/datawhalechina/handy-ollama\" target=\"_blank\" rel=\"noopener noreferrer\">handy-ollama</a></h2>\n<h3>项目简介</h3>\n<p>动手学 Ollama 教程，轻松上手实现大模型本地化部署，快速在本地管理以及运行大模型，让 CPU 也可以玩转大模型推理部署！</p>\n<p>本教程涵盖从基础入门到进阶使用的全方位内容，并通过实际应用案例深入理解和掌握大模型部署以及应用技术。我们的教程提供清晰的步骤和实用的技巧，无论是刚刚接触大模型部署的小白，还是有一定经验的开发者，都可以从零开始学习 Ollama ，实现本地部署大模型以及相关应用。</p>\n<p>目录结构说明：</p>\n<p>docs ---------------------- Markdown 文档文件\nnotebook ------------------ Notebook 源代码文件以及部分 Python、Java 和 JavaScript 源文件\nimages -------------------- 图片\n在线阅读：https://datawhalechina.github.io/handy-ollama/</p>\n<h2><a class=\"header-anchor\" href=\"#machine-learning-toy-code\"><span></span></a><a href=\"https://github.com/datawhalechina/machine-learning-toy-code\" target=\"_blank\" rel=\"noopener noreferrer\">machine-learning-toy-code</a></h2>\n<p>西瓜书代码实战</p>\n<p>本项目以西瓜书以及南瓜书为主要参考，其他资料为辅助，来进行常见机器学习代码的实战。主要特色为力求数码结合，即数学公式与相关代码的形神对应，能够帮助读者加深对公式的理解以及代码的熟练。</p>\n<h2><a class=\"header-anchor\" href=\"#so-large-lm\"><span></span></a><a href=\"https://github.com/datawhalechina/so-large-lm\" target=\"_blank\" rel=\"noopener noreferrer\">so-large-lm</a></h2>\n<h3>项目简介</h3>\n<p>本项目旨在作为一个大规模预训练语言模型的教程，从数据准备、模型构建、训练策略到模型评估与改进，以及模型在安全、隐私、环境和法律道德方面的方面来提供开源知识。</p>\n<p>项目将以斯坦福大学大规模语言模型课程和李宏毅生成式AI课程为基础，结合来自开源贡献者的补充和完善，以及对前沿大模型知识的及时更新，为读者提供较为全面而深入的理论知识和实践方法。通过对模型构建、训练、评估与改进等方面的系统性讲解，以及代码的实战，我们希望建立一个具有广泛参考价值的项目。</p>\n<p>我们的项目团队成员将分工负责各个章节的内容梳理和撰写，并预计在三个月内完成初始版本内容。随后，我们将持续根据社区贡献和反馈进行内容的更新和优化，以确保项目的持续发展和知识的时效性。我们期待通过这个项目，为大型语言模型研究领域贡献一份宝贵的资源，推动相关技术的快速发展和广泛应用。</p>\n</div>","r":{"minutes":8.64,"words":2591},"t":"学习资料","y":"a"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/%E5%B7%A5%E5%85%B7.html", { loader: () => import(/* webpackChunkName: "技术科普_工具.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/工具.html.js"), meta: {"d":1721865600000,"l":"2024年7月25日","c":["工具"],"g":["skill"],"e":"<h2>ubuntu</h2>\n<h3>ping 安装</h3>\n<div class=\"language- line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"\" data-title=\"\" style=\"--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34\"><pre class=\"shiki shiki-themes github-light one-dark-pro vp-code\"><code><span class=\"line\"><span>apt install iputils-ping</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div>","r":{"minutes":0.2,"words":61},"t":"常用工具总结","y":"a"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/%E5%BC%80%E6%BA%90%E6%8A%80%E6%9C%AF.html", { loader: () => import(/* webpackChunkName: "技术科普_开源技术.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/开源技术.html.js"), meta: {"d":1720656000000,"l":"2024年7月11日","c":["开源技术"],"g":["推理引擎"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">MInference：通过动态稀疏注意力加速长上下文 LLM 的预填充</p>\n<h2>地址</h2>\n<p><a href=\"https://github.com/microsoft/MInference\" target=\"_blank\" rel=\"noopener noreferrer\">https://github.com/microsoft/MInference</a></p>\n<h2>论文</h2>\n<p><a href=\"https://hqjiang.com/minference.html\" target=\"_blank\" rel=\"noopener noreferrer\">https://hqjiang.com/minference.html</a></p>\n<h2>介绍</h2>\n<p>论文介绍了一种名为 MInference 的动态稀疏注意力方法，用于解决长上下文 LLM 推理中的问题，并通过多种实验和测试展示了其效果。</p>\n<h2>重要亮点</h2>\n<ul>\n<li>\n<p>MInference 的提出背景：长上下文 LLM 推理面临预填充阶段注意力延迟长、KV 缓存存储和传输成本高等挑战，之前方法难以低成本在单个 A100 GPU 实现百万级标记提示的可接受延迟，MInference 应运而生。</p>\n</li>\n<li>\n<p>MInference 的工作原理：利用动态稀疏注意的静态空间聚合模式，离线确定每个头的最佳动态稀疏模式，在推理中动态近似动态稀疏索引，使用优化的 GPU 内核执行高效计算，显著减少预填充阶段延迟。</p>\n</li>\n<li>\n<p>MInference 的主要贡献：加速长上下文 LLM 的预填充阶段多达 10 倍；将动态稀疏注意力分为三种模式并设计搜索算法；引入在线近似方法和优化内核，提出最佳推理代码库；通过四个基准测试评估，在成本效率和系统延迟方面表现出色。</p>\n</li>\n<li>\n<p>长上下文基准测试中的实验结果：在问答、编码、基于检索等一系列任务中测试 MInference，有效保留或扩展实际上下文窗口处理能力，在不同模型和方法对比中性能良好。</p>\n</li>\n<li>\n<p>内核中的延迟细分和稀疏模式：展示三种注意力模式和 FlashAttention 的微基准测试结果，Vertical-Slash 虽慢但仍有显著加速，还展示了 Vertical-Slash 头部 kernel 中的稀疏索引。</p>\n</li>\n</ul>\n</div>","r":{"minutes":1.59,"words":478},"t":"开源技术","y":"a"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html", { loader: () => import(/* webpackChunkName: "技术科普_操作系统.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/操作系统.html.js"), meta: {"d":1723507200000,"l":"2024年8月13日","c":["操作系统"],"g":["操作系统"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">strace命令</p>\n</div>\n<p><code>strace</code> 是一个用于跟踪系统调用和信号的工具，它可以帮助你诊断程序的行为或调试程序。<code>-p</code> 选项用于附加到一个正在运行的进程上，并实时显示它的系统调用。</p>\n<h3>解释命令：</h3>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" data-title=\"bash\" style=\"--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34\"><pre class=\"shiki shiki-themes github-light one-dark-pro vp-code\"><code><span class=\"line\"><span style=\"--shiki-light:#6F42C1;--shiki-dark:#61AFEF\">strace</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> -p</span><span style=\"--shiki-light:#005CC5;--shiki-dark:#D19A66\"> 871</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div>","r":{"minutes":6.77,"words":2030},"t":"操作系统","y":"a"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C.html", { loader: () => import(/* webpackChunkName: "技术科普_计算机网络.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/计算机网络.html.js"), meta: {"d":1720396800000,"l":"2024年7月8日","c":["网络"],"g":["http"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">http2与http1区别?</p>\n</div>\n<p>HTTP/2 是对 HTTP/1.x 协议的重大升级，旨在提高网络性能和效率。以下是 HTTP/2 相对于 HTTP/1 的一些关键区别和改进：</p>\n<h3>1. 协议基础</h3>\n<ul>\n<li><strong>HTTP/1.x</strong>: 基于纯文本的协议，使用新行符分隔的文本消息进行通信。</li>\n<li><strong>HTTP/2</strong>: 基于二进制的协议，使用二进制帧传输数据，更高效地解析和传输数据。</li>\n</ul>","r":{"minutes":24.55,"words":7366},"t":"计算机网络","y":"a"} }],
  ["/%E6%9D%82%E8%B0%88/%E7%8B%AC%E7%AB%8B%E5%BC%80%E5%8F%91%E8%80%85.html", { loader: () => import(/* webpackChunkName: "杂谈_独立开发者.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/杂谈/独立开发者.html.js"), meta: {"d":1718064000000,"l":"2024年6月11日","c":["杂谈"],"g":["独立开发者"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">杂谈之独立开发者一点思考</p>\n</div>\n<h1>博文1</h1>\n<p><a href=\"https://indiehacker.one/\" target=\"_blank\" rel=\"noopener noreferrer\">https://indiehacker.one/</a></p>\n<p><strong>全文总结</strong>\n本文主要介绍了独立开发者的相关内容，包括什么是独立开发者、为什么要做独立开发者、有哪些牛逼的独立开发者、独立开发者面临的困境、独立开发的一些基本流程、想法、如何判断想法的是不是可以赚钱、三大核心问题、如何解决用户的问题、使用什么样的编程语言、采用什么样的产品形态、一个 MVP 产品最少需要做多少工作、一周开发一个新产品、三点金规铁律、发布、别人说你的产品是垃圾，伪需求怎么办、增长、心态、如何保持积极的心态、如何面对压力等。</p>","r":{"minutes":6.21,"words":1862},"t":"独立开发者一点思考","y":"a"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/%E8%AE%BA%E6%96%87/MPress.html", { loader: () => import(/* webpackChunkName: "技术科普_论文_MPress.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/论文/MPress.html.js"), meta: {"d":1725926400000,"l":"2024年9月10日","c":["MPress"],"g":["MPress","GPU"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism</p>\n<p>地址：<a href=\"https://par.nsf.gov/servlets/purl/10410479\" target=\"_blank\" rel=\"noopener noreferrer\">https://par.nsf.gov/servlets/purl/10410479</a></p>\n</div>","r":{"minutes":36.19,"words":10856},"t":"MPress 通过存储保存算子间并行性在多GPU服务器上实现十亿规模级模型训练的民主化","y":"a"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/%E8%AE%BA%E6%96%87/nnScaler.html", { loader: () => import(/* webpackChunkName: "技术科普_论文_nnScaler.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/论文/nnScaler.html.js"), meta: {"d":1726012800000,"l":"2024年9月11日","c":["nnScaler"],"g":["nnScaler","GPU"],"e":"<div class=\"hint-container tip\">\n<p class=\"hint-container-title\">MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism</p>\n<p>地址：<a href=\"https://www.usenix.org/system/files/osdi24-lin-zhiqi.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.usenix.org/system/files/osdi24-lin-zhiqi.pdf</a></p>\n<p>中文解读：<a href=\"https://mp.weixin.qq.com/s/GV_CF9fPpxsPBNbEsvhS5g\" target=\"_blank\" rel=\"noopener noreferrer\">https://mp.weixin.qq.com/s/GV_CF9fPpxsPBNbEsvhS5g</a></p>\n</div>","r":{"minutes":38.61,"words":11583},"t":"nnScaler：重塑深度学习并行策略，大幅提升训练效率","y":"a"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/%E8%AE%BA%E6%96%87/", { loader: () => import(/* webpackChunkName: "技术科普_论文_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/论文/index.html.js"), meta: {"d":1726023315000,"r":{"minutes":0.03,"words":10},"t":"论文解读","O":1,"y":"a"} }],
  ["/404.html", { loader: () => import(/* webpackChunkName: "404.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/404.html.js"), meta: {"t":""} }],
  ["/%E4%BA%91%E5%8E%9F%E7%94%9F/", { loader: () => import(/* webpackChunkName: "云原生_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/云原生/index.html.js"), meta: {"t":"云原生"} }],
  ["/%E4%BB%8B%E7%BB%8D/", { loader: () => import(/* webpackChunkName: "介绍_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/介绍/index.html.js"), meta: {"t":"介绍"} }],
  ["/%E5%B7%A5%E5%85%B7/", { loader: () => import(/* webpackChunkName: "工具_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/工具/index.html.js"), meta: {"t":"工具"} }],
  ["/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/", { loader: () => import(/* webpackChunkName: "技术科普_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/index.html.js"), meta: {"t":"技术科普"} }],
  ["/%E6%9D%82%E8%B0%88/", { loader: () => import(/* webpackChunkName: "杂谈_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/杂谈/index.html.js"), meta: {"t":"杂谈"} }],
  ["/category/", { loader: () => import(/* webpackChunkName: "category_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/index.html.js"), meta: {"t":"分类","I":false} }],
  ["/category/etcd/", { loader: () => import(/* webpackChunkName: "category_etcd_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/etcd/index.html.js"), meta: {"t":"etcd 分类","I":false} }],
  ["/category/k8s/", { loader: () => import(/* webpackChunkName: "category_k8s_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/k8s/index.html.js"), meta: {"t":"k8s 分类","I":false} }],
  ["/category/kong/", { loader: () => import(/* webpackChunkName: "category_kong_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/kong/index.html.js"), meta: {"t":"kong 分类","I":false} }],
  ["/category/%E4%BA%91%E5%8E%9F%E7%94%9F/", { loader: () => import(/* webpackChunkName: "category_云原生_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/云原生/index.html.js"), meta: {"t":"云原生 分类","I":false} }],
  ["/category/%E4%B8%AA%E4%BA%BA%E4%BB%8B%E7%BB%8D/", { loader: () => import(/* webpackChunkName: "category_个人介绍_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/个人介绍/index.html.js"), meta: {"t":"个人介绍 分类","I":false} }],
  ["/category/%E4%BB%A3%E7%90%86/", { loader: () => import(/* webpackChunkName: "category_代理_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/代理/index.html.js"), meta: {"t":"代理 分类","I":false} }],
  ["/category/aigc/", { loader: () => import(/* webpackChunkName: "category_aigc_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/aigc/index.html.js"), meta: {"t":"AIGC 分类","I":false} }],
  ["/category/%E5%88%86%E5%B8%83%E5%BC%8F/", { loader: () => import(/* webpackChunkName: "category_分布式_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/分布式/index.html.js"), meta: {"t":"分布式 分类","I":false} }],
  ["/category/%E8%B5%84%E6%96%99/", { loader: () => import(/* webpackChunkName: "category_资料_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/资料/index.html.js"), meta: {"t":"资料 分类","I":false} }],
  ["/category/%E5%B7%A5%E5%85%B7/", { loader: () => import(/* webpackChunkName: "category_工具_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/工具/index.html.js"), meta: {"t":"工具 分类","I":false} }],
  ["/category/%E5%BC%80%E6%BA%90%E6%8A%80%E6%9C%AF/", { loader: () => import(/* webpackChunkName: "category_开源技术_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/开源技术/index.html.js"), meta: {"t":"开源技术 分类","I":false} }],
  ["/category/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/", { loader: () => import(/* webpackChunkName: "category_操作系统_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/操作系统/index.html.js"), meta: {"t":"操作系统 分类","I":false} }],
  ["/category/%E7%BD%91%E7%BB%9C/", { loader: () => import(/* webpackChunkName: "category_网络_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/网络/index.html.js"), meta: {"t":"网络 分类","I":false} }],
  ["/category/%E6%9D%82%E8%B0%88/", { loader: () => import(/* webpackChunkName: "category_杂谈_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/杂谈/index.html.js"), meta: {"t":"杂谈 分类","I":false} }],
  ["/category/mpress/", { loader: () => import(/* webpackChunkName: "category_mpress_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/mpress/index.html.js"), meta: {"t":"MPress 分类","I":false} }],
  ["/category/nnscaler/", { loader: () => import(/* webpackChunkName: "category_nnscaler_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/category/nnscaler/index.html.js"), meta: {"t":"nnScaler 分类","I":false} }],
  ["/tag/", { loader: () => import(/* webpackChunkName: "tag_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/index.html.js"), meta: {"t":"标签","I":false} }],
  ["/tag/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/", { loader: () => import(/* webpackChunkName: "tag_分布式锁_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/分布式锁/index.html.js"), meta: {"t":"标签: 分布式锁","I":false} }],
  ["/tag/%E7%AC%94%E8%AE%B0/", { loader: () => import(/* webpackChunkName: "tag_笔记_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/笔记/index.html.js"), meta: {"t":"标签: 笔记","I":false} }],
  ["/tag/kong/", { loader: () => import(/* webpackChunkName: "tag_kong_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/kong/index.html.js"), meta: {"t":"标签: kong","I":false} }],
  ["/tag/nginx/", { loader: () => import(/* webpackChunkName: "tag_nginx_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/nginx/index.html.js"), meta: {"t":"标签: nginx","I":false} }],
  ["/tag/html/", { loader: () => import(/* webpackChunkName: "tag_html_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/html/index.html.js"), meta: {"t":"标签: html","I":false} }],
  ["/tag/kubeflow/", { loader: () => import(/* webpackChunkName: "tag_kubeflow_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/kubeflow/index.html.js"), meta: {"t":"标签: Kubeflow","I":false} }],
  ["/tag/operator/", { loader: () => import(/* webpackChunkName: "tag_operator_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/operator/index.html.js"), meta: {"t":"标签: operator","I":false} }],
  ["/tag/%E4%B8%AA%E4%BA%BA/", { loader: () => import(/* webpackChunkName: "tag_个人_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/个人/index.html.js"), meta: {"t":"标签: 个人","I":false} }],
  ["/tag/clash/", { loader: () => import(/* webpackChunkName: "tag_clash_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/clash/index.html.js"), meta: {"t":"标签: clash","I":false} }],
  ["/tag/v2ray/", { loader: () => import(/* webpackChunkName: "tag_v2ray_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/v2ray/index.html.js"), meta: {"t":"标签: v2ray","I":false} }],
  ["/tag/fastgpt/", { loader: () => import(/* webpackChunkName: "tag_fastgpt_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/fastgpt/index.html.js"), meta: {"t":"标签: FastGpt","I":false} }],
  ["/tag/chatgpt-on-web/", { loader: () => import(/* webpackChunkName: "tag_chatgpt-on-web_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/chatgpt-on-web/index.html.js"), meta: {"t":"标签: chatgpt-on-web","I":false} }],
  ["/tag/pytorch/", { loader: () => import(/* webpackChunkName: "tag_pytorch_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/pytorch/index.html.js"), meta: {"t":"标签: PyTorch","I":false} }],
  ["/tag/c10d/", { loader: () => import(/* webpackChunkName: "tag_c10d_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/c10d/index.html.js"), meta: {"t":"标签: c10d","I":false} }],
  ["/tag/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/", { loader: () => import(/* webpackChunkName: "tag_学习资料_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/学习资料/index.html.js"), meta: {"t":"标签: 学习资料","I":false} }],
  ["/tag/skill/", { loader: () => import(/* webpackChunkName: "tag_skill_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/skill/index.html.js"), meta: {"t":"标签: skill","I":false} }],
  ["/tag/%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/", { loader: () => import(/* webpackChunkName: "tag_推理引擎_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/推理引擎/index.html.js"), meta: {"t":"标签: 推理引擎","I":false} }],
  ["/tag/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/", { loader: () => import(/* webpackChunkName: "tag_操作系统_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/操作系统/index.html.js"), meta: {"t":"标签: 操作系统","I":false} }],
  ["/tag/http/", { loader: () => import(/* webpackChunkName: "tag_http_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/http/index.html.js"), meta: {"t":"标签: http","I":false} }],
  ["/tag/%E7%8B%AC%E7%AB%8B%E5%BC%80%E5%8F%91%E8%80%85/", { loader: () => import(/* webpackChunkName: "tag_独立开发者_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/独立开发者/index.html.js"), meta: {"t":"标签: 独立开发者","I":false} }],
  ["/tag/mpress/", { loader: () => import(/* webpackChunkName: "tag_mpress_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/mpress/index.html.js"), meta: {"t":"标签: MPress","I":false} }],
  ["/tag/gpu/", { loader: () => import(/* webpackChunkName: "tag_gpu_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/gpu/index.html.js"), meta: {"t":"标签: GPU","I":false} }],
  ["/tag/nnscaler/", { loader: () => import(/* webpackChunkName: "tag_nnscaler_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/tag/nnscaler/index.html.js"), meta: {"t":"标签: nnScaler","I":false} }],
  ["/article/", { loader: () => import(/* webpackChunkName: "article_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/article/index.html.js"), meta: {"t":"文章","I":false} }],
  ["/star/", { loader: () => import(/* webpackChunkName: "star_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/star/index.html.js"), meta: {"t":"星标","I":false} }],
  ["/timeline/", { loader: () => import(/* webpackChunkName: "timeline_index.html" */"D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/timeline/index.html.js"), meta: {"t":"时间轴","I":false} }],
]);
