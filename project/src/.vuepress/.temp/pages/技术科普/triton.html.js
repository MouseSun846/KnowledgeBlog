import comp from "D:/Code/knowledgeblob/project/src/.vuepress/.temp/pages/技术科普/triton.html.vue"
const data = JSON.parse("{\"path\":\"/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/triton.html\",\"title\":\"Triton Inference Server\",\"lang\":\"zh-CN\",\"frontmatter\":{\"date\":\"2024-10-22T00:00:00.000Z\",\"title\":\"Triton Inference Server\",\"category\":[\"AI Framework\"],\"tag\":[\"Triton\"],\"description\":\"::: Triton Inference Server ::: 指标解释 参考 平均请求在队列中等待的时间百分比（不包括缓存命中）： expr: 使用 rate(nv_inference_queue_duration_us[1m]) 计算在过去1分钟内，推理请求在队列中等待的时间的速率。 clamp_min(rate(nv_inference_comp...\",\"head\":[[\"meta\",{\"property\":\"og:url\",\"content\":\"https://mousesun846.github.io/KnowledgeBlog/KnowledgeBlog/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/triton.html\"}],[\"meta\",{\"property\":\"og:site_name\",\"content\":\"知识笔记\"}],[\"meta\",{\"property\":\"og:title\",\"content\":\"Triton Inference Server\"}],[\"meta\",{\"property\":\"og:description\",\"content\":\"::: Triton Inference Server ::: 指标解释 参考 平均请求在队列中等待的时间百分比（不包括缓存命中）： expr: 使用 rate(nv_inference_queue_duration_us[1m]) 计算在过去1分钟内，推理请求在队列中等待的时间的速率。 clamp_min(rate(nv_inference_comp...\"}],[\"meta\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"meta\",{\"property\":\"og:locale\",\"content\":\"zh-CN\"}],[\"meta\",{\"property\":\"article:author\",\"content\":\"MouseSun\"}],[\"meta\",{\"property\":\"article:tag\",\"content\":\"Triton\"}],[\"meta\",{\"property\":\"article:published_time\",\"content\":\"2024-10-22T00:00:00.000Z\"}],[\"script\",{\"type\":\"application/ld+json\"},\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Article\\\",\\\"headline\\\":\\\"Triton Inference Server\\\",\\\"image\\\":[\\\"\\\"],\\\"datePublished\\\":\\\"2024-10-22T00:00:00.000Z\\\",\\\"dateModified\\\":null,\\\"author\\\":[{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"MouseSun\\\",\\\"url\\\":\\\"https://github.com/MouseSun846\\\",\\\"email\\\":\\\"\\\"}]}\"]]},\"headers\":[{\"level\":2,\"title\":\"指标解释\",\"slug\":\"指标解释\",\"link\":\"#指标解释\",\"children\":[]},{\"level\":2,\"title\":\"HPA\",\"slug\":\"hpa\",\"link\":\"#hpa\",\"children\":[]}],\"git\":{\"createdTime\":null,\"updatedTime\":null,\"contributors\":[]},\"readingTime\":{\"minutes\":1.35,\"words\":405},\"filePathRelative\":\"技术科普/triton.md\",\"localizedDate\":\"2024年10月22日\",\"excerpt\":\"<p>::: Triton Inference Server\\n:::</p>\\n<h2>指标解释</h2>\\n<p><a href=\\\"https://github1s.com/triton-inference-server/tutorials/blob/main/Deployment/Kubernetes/EKS_Multinode_Triton_TRTLLM/multinode_helm_chart/triton-metrics_prometheus-rule.yaml#L38\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">参考</a></p>\\n<div class=\\\"language- line-numbers-mode\\\" data-highlighter=\\\"shiki\\\" data-ext=\\\"\\\" data-title=\\\"\\\" style=\\\"--shiki-light:#24292e;--shiki-dark:#abb2bf;--shiki-light-bg:#fff;--shiki-dark-bg:#282c34\\\"><pre class=\\\"shiki shiki-themes github-light one-dark-pro vp-code\\\"><code><span class=\\\"line\\\"><span># Average percentage of time inference requests spend in queue (not including cache hits).</span></span>\\n<span class=\\\"line\\\"><span>- expr: rate(nv_inference_queue_duration_us[1m])/clamp_min(rate(nv_inference_compute_infer_duration_us[1m]),1)</span></span>\\n<span class=\\\"line\\\"><span>record: triton:queue_compute:ratio</span></span></code></pre>\\n<div class=\\\"line-numbers\\\" aria-hidden=\\\"true\\\" style=\\\"counter-reset:line-number 0\\\"><div class=\\\"line-number\\\"></div><div class=\\\"line-number\\\"></div><div class=\\\"line-number\\\"></div></div></div>\",\"autoDesc\":true}")
export { comp, data }
