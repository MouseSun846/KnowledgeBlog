import{_ as e,c as n,o as a,a as r}from"./app-BQgatZkG.js";const t={},p=r('<div class="hint-container tip"><p class="hint-container-title">MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism</p><p>地址：<a href="https://www.usenix.org/system/files/osdi24-lin-zhiqi.pdf" target="_blank" rel="noopener noreferrer">https://www.usenix.org/system/files/osdi24-lin-zhiqi.pdf</a> 中文解读：<a href="https://mp.weixin.qq.com/s/GV_CF9fPpxsPBNbEsvhS5g" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/GV_CF9fPpxsPBNbEsvhS5g</a></p></div><h3 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h3><p>随着深度神经网络（DNN）模型规模的增长，深度学习训练越来越依赖手工设计的搜索空间来找到高效的并行执行计划。然而，我们的研究表明，现有的搜索空间忽略了一些重要的计划配置，这些配置在某些设置下（如处理大型嵌入表时）对著名DNN模型（例如AlphaFold2）的训练性能有显著影响。</p><p>为了解决这个问题，我们提出了nnScaler，这是一个用于生成深度学习训练并行化计划的框架。nnScaler不依赖现有的搜索空间，而是通过三个原语（op-trans、op-assign和op-order），让领域专家能够构建自己的搜索空间。这些原语能够捕捉模型的转换以及任何并行化计划中转化模型的时空调度。为了避免搜索空间爆炸，nnScaler允许在构建空间时对这些原语应用约束。通过这些原语和约束，nnScaler不仅可以构建现有的搜索空间，还可以创建新的空间。实验表明，nnScaler能够在新的搜索空间中找到并行化计划，与DeepSpeed、Megatron-LM和Alpa等解决方案相比，对一些流行的DNN模型（如Swin-Transformer和AlphaFold2）实现了最高3.5倍的加速。</p><h3 id="_1-引言" tabindex="-1"><a class="header-anchor" href="#_1-引言"><span>1 引言</span></a></h3><p>近年来，深度神经网络（DNN）模型的规模迅速增长，训练这些模型的计算需求也随之大幅增加。为了满足这一需求，分布式训练成为了主流。分布式训练通过将计算任务分配到多个设备（通常是GPU）上来加速训练过程。这种并行化的效果依赖于如何将模型的计算操作有效地分配给多个设备，合理的分配计划可以显著提高训练性能。</p><p>然而，设计高效的并行训练计划并非易事。现有的方法通常依赖于手工设计的搜索空间，这些搜索空间定义了并行化的各种配置。然而，我们的研究表明，这些搜索空间通常不够全面，忽略了许多潜在的计划配置。这些遗漏的配置在某些情况下对训练性能有很大的影响，尤其是在处理大型嵌入表或复杂模型（如AlphaFold2）时。</p><p>为了填补这一空白，我们提出了nnScaler，一个新的框架，用于生成深度学习训练的并行化计划。nnScaler的核心思想是通过三个基本原语（op-trans、op-assign、op-order）让领域专家能够定义和构建自己的搜索空间。这些原语可以捕捉模型转换以及任何并行化计划中对模型的时空调度，从而避免了现有方法中过于狭窄的搜索空间。</p><p>nnScaler不仅可以重现现有的搜索空间，还可以创建新的搜索空间，从而探索更多潜在的并行化配置。通过在构建空间时引入约束条件，nnScaler有效地避免了搜索空间爆炸的问题。</p><p>我们的实验结果表明，nnScaler能够在这些新的搜索空间中找到高效的并行化计划，并在多个流行的DNN模型（如Swin-Transformer和AlphaFold2）上取得了显著的性能提升，最高加速达到3.5倍，优于现有的主流解决方案（如DeepSpeed、Megatron-LM和Alpa）。</p><h3 id="_2、背景与动机" tabindex="-1"><a class="header-anchor" href="#_2、背景与动机"><span>2、背景与动机</span></a></h3><p>并行化计划的搜索空间。并行化计划是指一种训练执行计划，它指定了在给定的 GPU 集合上模型的分区和相应的时空调度方案。训练一个拥有数百亿参数的大型模型需要数千个 GPU。一个大型模型可能由大约 100 层组成，每一层代表一个子神经架构（例如，注意力机制），其中包含处理具有数万维度大小的张量的多个操作符（例如，隐藏维度）。对于大型模型而言，广泛的分区选择和大量的时空调度选择结合在一起，形成了一个极其庞大且组合复杂的并行化计划搜索空间。</p><p>现有的方法依赖于经过充分研究的手工并行化计划或搜索空间来解决这个问题。例如，数据并行性是一种特殊的并行化计划，它沿着与其相关的张量的批次维度对操作符进行分区。这些分区后的操作符随后在多个设备（GPU）上复制，并共享相同的模型参数（权重），以实现并发模型训练。张量并行性是一类更一般的计划，允许在不限于批次维度的维度上进行分区。这种方法允许将分区后的操作符分布在不同的设备上，以适应无法在单个设备上容纳的模型。</p><p>由于大型深度神经网络（DNN）模型通常由多个层组成，因此也可以将模型分为多个阶段，每个阶段包含一个或多个层。各个阶段被放置在不同的设备上并以流水线方式执行，因此称为流水线并行性。为了提高流水线效率，训练样本的批次进一步被划分为微批次，并按照精心设计的时间顺序执行。</p><p>上述并行化方案可以组合成一种新的方案，称为 3D 并行性，以进一步提高训练效率。Megatron-LM 集成了 3D 并行性，这种方法以参数化的方式结合了数据、张量和流水线并行性，以支持类似 GPT 的模型。给定 N 个设备，Megatron-LM 将模型分为 K 个阶段，每个阶段再分为 M 个分区。模型使用 K 阶段流水线并行性和 M 路张量并行性进行执行。对于足够大的 N，Megatron-LM 还可以采用 (N / (M * K))-路数据并行性，以进一步提高训练性能。3D 并行性代表了在大型搜索空间内几类经过充分研究的并行化计划。</p><p>Alpa 进一步将这些并行化方案进行概括，手工构建了一个两级层次化搜索空间。这个层次结构使得可以使用动态规划等高效搜索技术。由于其更大的搜索空间，即 SPMD（广义张量并行空间）和流水线并行性的结合，Alpa 被证明能够产生更优的并行化计划。</p><h3 id="现有搜索空间的局限性" tabindex="-1"><a class="header-anchor" href="#现有搜索空间的局限性"><span>现有搜索空间的局限性</span></a></h3><p>尽管现有的手工并行化搜索空间在具有相似模型架构的主流模型中显示出有效性，但它依赖于简化搜索和构建并行化计划的假设。然而，这些简化可能会排除一些有前景的计划。</p><p>在张量并行性中，假设分区后的操作符及其对应的分割张量分布在不相交的设备上。例如，为了训练具有高保真图像的视觉模型，张量并行性将与大图像相关的大型张量进行分割，并将分割后的张量分配给不相交的设备。这排除了将分割操作符放置在较少设备上的情况，即多个操作符共享一个设备，并以流线化的方式计算，以同时减少内存消耗和设备间通信成本。</p><p>流水线并行性假设训练涉及一次前向传播和一次反向传播。然而，像 AlphaFold2 这样的模型需要三次前向传播和一次反向传播。这种非常规的训练方法使得现有的流水线并行性无法适用。</p><p>流水线并行性还假设不同的流水线阶段分布在不相交的设备上，并禁止任何两个阶段通过时间复用共享相同的设备集。例如，多语言大型语言模型（LLMs）通常在模型的早期计算阶段使用一个大型嵌入表。这导致显著的 GPU 内存消耗（超过 40%），但计算利用率却很低（不到 5%）。鉴于流水线并行性（以及张量并行性）中的不相交设备分配，硬件利用率的不平衡是不可避免的。</p><p>后来的手工搜索空间（例如，结合张量和流水线并行性等的方案）继承了这些假设，因此也遭受了相同的局限性。这促使我们设计一种更灵活的空间构建方法，使领域专家能够为他们的模型找到更有效的训练计划。</p><h3 id="由于灵活性带来的新挑战" tabindex="-1"><a class="header-anchor" href="#由于灵活性带来的新挑战"><span>由于灵活性带来的新挑战</span></a></h3><p>引入一种更灵活的方式来构建并行化计划空间带来了新的挑战。现有的框架，如 Megatron-LM、Alpa 和 DeepSpeed，仅实现了一些经过充分研究的分区、调度和通信方案，这些方案支持在已知的并行化空间内的并行化计划。然而，新的空间可能会揭示操作符分区的新方法，以及具有非常规通信模式的新操作符调度。此外，更灵活的并行化计划研究较少，因此可能容易出错。</p><p>为了解决上述挑战，我们设计了一个编译过程，以检测和防止并行化计划中的潜在错误（例如，转换后的数据流图中的循环），并为发现的并行化计划生成具有高效通信操作的运行时代码。</p><h3 id="_3-并行化搜索空间构建" tabindex="-1"><a class="header-anchor" href="#_3-并行化搜索空间构建"><span>3. 并行化搜索空间构建</span></a></h3><p>并行化计划可以自然地通过模型分区和分区模型的时空调度来表达。因此，nnScaler 提出了三个原语：op-trans、op-assign 和 op-order（见表 1），以捕捉并行化计划的三个方面。将这些原语结合起来，可以为任意模型和加速器设备构建任何并行化计划的搜索空间。</p><h4 id="op-trans" tabindex="-1"><a class="header-anchor" href="#op-trans"><span>op-trans</span></a></h4><p>op-trans(op, algo, n) 根据选定的转换算法 algo 将操作符 op 转换为 n 个子操作符，该算法从与 op 类型相对应的算法集中选择。例如，矩阵乘法操作符 matmul(Ai,k,Bk,j) 可以沿着张量 A 的维度 i 将其分区为两个 matmul 操作符，同时复制张量 B。实际上，大多数操作符可以沿着其相关张量的某个维度（例如 A 或 B 的 i 或 k）进行分区，并且分区后的（子）操作符的计算与原始操作符的计算保持一致。</p><p>基于这一观察，nnScaler 实现了大多数深度神经网络（DNN）模型中主要操作符的分区算法。领域专家可以通过 algos() 接口重用所需的算法。nnScaler 还可以集成自定义转换算法，例如由领域专家开发的算法，适用于任何给定的操作符。需要注意的是，转换算法不仅限于操作符分区。例如，可以通过增加一个额外的重新计算操作符或内存交换操作符来增强操作符，以节省内存。在本文中，我们将“转换”和“分区”这两个术语互换使用。</p><h4 id="op-assign" tabindex="-1"><a class="header-anchor" href="#op-assign"><span>op-assign</span></a></h4><p>给定一组设备 D 和一个操作符 op，op-assign(op, d) 表示操作符 op 将在 D 中的第 d 个设备上执行。</p><h4 id="op-order" tabindex="-1"><a class="header-anchor" href="#op-order"><span>op-order</span></a></h4><p>当非依赖操作符（例如 op1 和 op2）被分配到同一设备时，op-order(op1, op2) 确保op1 必须在 op2 之前执行。非依赖操作符的执行顺序在训练性能中可以发挥至关重要的作用。例如，在流水线并行性中，流水线阶段中的一个操作符可以沿着批次维度分区成多个微批次。我们把这些（子）操作符表示为 op.mb1、op.mb2 等，其中 mbi 表示相应的微批次 ID。这些操作符 op.mbi 可以在任意顺序下相对于 op.mbj（i ≠ j）执行。</p><p>然而，各种研究表明，一旦这些操作符在时间维度上被精心编排，就有可能最小化流水线“气泡” [24, 54]，从而显著提高训练效率。</p><p>使用上述提到的三个原语，领域专家可以编写 Python 代码来为任何深度神经网络（DNN）模型构建任意并行化计划的搜索空间。这些代码不一定与特定的 DNN 模型绑定。因此，nnScaler 将模型代码与搜索空间和搜索策略相关的代码分离。请注意，为了简化编程工作，原语中的 op 可以代表一个子图，其中原语适用于子图中的每个操作符。</p><p>由于原语的灵活性和大型 DNN 模型的规模，构建的并行化搜索空间通常包含数百甚至数千个操作符，具有组合搜索复杂性。为了解决这个问题，nnScaler 允许领域专家在应用这些原语时施加约束。这些约束可以显著减少搜索空间（第 4 节），从而使得有效的搜索方法（第 5 节）成为可能。</p>',37),o=[p];function i(l,s){return a(),n("div",null,o)}const h=e(t,[["render",i],["__file","nnScaler.html.vue"]]),d=JSON.parse('{"path":"/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/%E8%AE%BA%E6%96%87/nnScaler.html","title":"nnScaler：重塑深度学习并行策略，大幅提升训练效率","lang":"zh-CN","frontmatter":{"date":"2024-09-11T00:00:00.000Z","title":"nnScaler：重塑深度学习并行策略，大幅提升训练效率","category":["nnScaler"],"tag":["nnScaler","GPU"],"description":"MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism 地址：https://www.usenix.org/system/files/osdi24-lin-zhiqi.pdf ...","head":[["meta",{"property":"og:url","content":"https://mousesun846.github.io/KnowledgeBlog/KnowledgeBlog/%E6%8A%80%E6%9C%AF%E7%A7%91%E6%99%AE/%E8%AE%BA%E6%96%87/nnScaler.html"}],["meta",{"property":"og:site_name","content":"知识笔记"}],["meta",{"property":"og:title","content":"nnScaler：重塑深度学习并行策略，大幅提升训练效率"}],["meta",{"property":"og:description","content":"MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism 地址：https://www.usenix.org/system/files/osdi24-lin-zhiqi.pdf ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-09-11T02:55:15.000Z"}],["meta",{"property":"article:author","content":"MouseSun"}],["meta",{"property":"article:tag","content":"nnScaler"}],["meta",{"property":"article:tag","content":"GPU"}],["meta",{"property":"article:published_time","content":"2024-09-11T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-09-11T02:55:15.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"nnScaler：重塑深度学习并行策略，大幅提升训练效率\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-09-11T00:00:00.000Z\\",\\"dateModified\\":\\"2024-09-11T02:55:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"MouseSun\\",\\"url\\":\\"https://github.com/MouseSun846\\",\\"email\\":\\"\\"}]}"]]},"headers":[{"level":3,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":3,"title":"1 引言","slug":"_1-引言","link":"#_1-引言","children":[]},{"level":3,"title":"2、背景与动机","slug":"_2、背景与动机","link":"#_2、背景与动机","children":[]},{"level":3,"title":"现有搜索空间的局限性","slug":"现有搜索空间的局限性","link":"#现有搜索空间的局限性","children":[]},{"level":3,"title":"由于灵活性带来的新挑战","slug":"由于灵活性带来的新挑战","link":"#由于灵活性带来的新挑战","children":[]},{"level":3,"title":"3. 并行化搜索空间构建","slug":"_3-并行化搜索空间构建","link":"#_3-并行化搜索空间构建","children":[]}],"git":{"createdTime":1726023315000,"updatedTime":1726023315000,"contributors":[{"name":"mousesun","email":"3026098675@qq.com","commits":1}]},"readingTime":{"minutes":11.45,"words":3436},"filePathRelative":"技术科普/论文/nnScaler.md","localizedDate":"2024年9月11日","excerpt":"<div class=\\"hint-container tip\\">\\n<p class=\\"hint-container-title\\">MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism</p>\\n<p>地址：<a href=\\"https://www.usenix.org/system/files/osdi24-lin-zhiqi.pdf\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://www.usenix.org/system/files/osdi24-lin-zhiqi.pdf</a>\\n中文解读：<a href=\\"https://mp.weixin.qq.com/s/GV_CF9fPpxsPBNbEsvhS5g\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://mp.weixin.qq.com/s/GV_CF9fPpxsPBNbEsvhS5g</a></p>\\n</div>","autoDesc":true}');export{h as comp,d as data};
